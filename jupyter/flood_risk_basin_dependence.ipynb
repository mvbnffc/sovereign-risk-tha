{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9c1baf-945a-444d-87b3-c24be624c80a",
   "metadata": {},
   "source": [
    "### Step 1: Do all the dependence analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd019295-54ef-41dc-9add-8dad9cd74dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import xarray as xr\n",
    "import netCDF4\n",
    "# CDS API\n",
    "import cdsapi\n",
    "# Disable warnings for data download via API\n",
    "import urllib3 \n",
    "urllib3.disable_warnings()\n",
    "# Disable xarray runtime warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lmoments3 import distr\n",
    "from scipy.stats import gumbel_r, kstest\n",
    "from copulas.bivariate import Clayton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6831a09-7414-4ff1-aa5f-8ad311c031b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate pairwise basin dependence\n",
    "# Set up CDS API key\n",
    "\n",
    "if os.path.isfile(\"C:/Users/Mark.DESKTOP-UFHIN6T/.cdsapirc\"):\n",
    "    cdsapi_kwargs = {}\n",
    "else:\n",
    "    URL = 'https://cds.climate.copernicus.eu/api/v2'\n",
    "    KEY = '##################################'\n",
    "    cdsapi_kwargs = {\n",
    "        'url': URL,\n",
    "        'key': KEY,\n",
    "    }\n",
    "\n",
    "# Set up data directory\n",
    "DATADIR = r\"D:\\projects\\sovereign-risk\\Thailand\\data\\flood\\dependence\\glofas\"\n",
    "os.makedirs(DATADIR, exist_ok=True)\n",
    "\n",
    "# Download data for Thaiand (the GIRI model uses the historical time period 1979-2016). We will pull data from 1979-2023\n",
    "start_year = 1979\n",
    "end_year = 2023\n",
    "# new_dir = os.path.join(DATADIR, f\"THA_{start_year}-{end_year}\")\n",
    "# os.makedirs(new_dir)\n",
    "c = cdsapi.Client(\n",
    "    **cdsapi_kwargs\n",
    "    )\n",
    "for year in range(start_year, end_year+1):\n",
    "    download_file = f\"{DATADIR}/glofas_THA_{year}.grib\"\n",
    "    if not os.path.isfile(download_file):\n",
    "        request_params = {\n",
    "            'system_version': 'version_4_0',\n",
    "            'hydrological_model': 'lisflood',\n",
    "            'product_type': 'consolidated',\n",
    "            'variable': 'river_discharge_in_the_last_24_hours',\n",
    "            'hyear': [f\"{year}\"],\n",
    "            'hmonth': ['january', 'february', 'march', 'april', 'may', 'june', \n",
    "                       'july', 'august', 'september', 'october', 'november', 'december'],\n",
    "            'hday': [f\"{day:02d}\" for day in range(1,31)],\n",
    "            'format': 'grib',\n",
    "            'area': [21, 97, 5, 106], # slightly larger bounding box than Thailand\n",
    "        }\n",
    "        c.retrieve('cems-glofas-historical', request_params).download(download_file)\n",
    "\n",
    "# Download the upstream area\n",
    "# NOTE: issue downloading a valid netcdf in current script. Workaround at the moment is using file I've previously downloaded\n",
    "upstream_area_fname = f\"uparea_glofas_v4_0.nc\"\n",
    "upstream_area_file = os.path.join(DATADIR, upstream_area_fname)\n",
    "# If we have not already downloaded the data, download it.\n",
    "if not os.path.isfile(upstream_area_file):\n",
    "    u_version=2 # file version\n",
    "    upstream_data_url = (\n",
    "        f\"https://confluence.ecmwf.int/download/attachments/242067380/{upstream_area_file}?\"\n",
    "        f\"version{u_version}&modificationDate=1668604690076&api=v2&download=true\"\n",
    "    )\n",
    "    import requests\n",
    "    result = requests.get(upstream_data_url)\n",
    "    with open(upstream_area_file, 'wb') as f:\n",
    "        f.write(result.content)\n",
    "\n",
    "def combine_datasets(start, end, dir):\n",
    "    '''\n",
    "    combine all datasets into one xarray for analysis\n",
    "    '''\n",
    "    all_files = [os.path.join(dir, f\"glofas_THA_{year}.grib\") for year in range(start, end+1)]\n",
    "    # Load all datasets into array\n",
    "    datasets = [xr.open_dataset(file, engine='cfgrib') for file in all_files]\n",
    "    # Concatenate all datasets along the time dimension\n",
    "    combined_dataset = xr.concat(datasets, dim='time')\n",
    "    # Make sure datasets are sorted by time\n",
    "    combined_dataset = combined_dataset.sortby('time')\n",
    "    \n",
    "    return combined_dataset\n",
    "\n",
    "# Load glofas data and combine\n",
    "glofas_data = combine_datasets(start_year, end_year, DATADIR)\n",
    "\n",
    "# Reduce the Upstream area data to the domain of the river discharge\n",
    "\n",
    "# Filter glofas timeseries based on upstream accumulating area\n",
    "area_filter = 500\n",
    "\n",
    "# Open the file and print the contents\n",
    "upstream_area = xr.open_dataset(upstream_area_file, engine='netcdf4')\n",
    "\n",
    "# Get the latitude and longitude limits of the data\n",
    "lat_limits = [glofas_data.latitude.values[i] for i in [0, -1]]\n",
    "lon_limits = [glofas_data.longitude.values[i] for i in [0, -1]]\n",
    "up_lats = upstream_area.latitude.values.tolist()\n",
    "up_lons = upstream_area.longitude.values.tolist()\n",
    "\n",
    "lat_slice_index = [\n",
    "    round((i-up_lats[0])/(up_lats[1]-up_lats[0]))\n",
    "    for i in lat_limits\n",
    "]\n",
    "lon_slice_index = [\n",
    "    round((i-up_lons[0])/(up_lons[1]-up_lons[0]))\n",
    "    for i in lon_limits\n",
    "]\n",
    "\n",
    "# Slice upstream area to Thailand region:\n",
    "red_upstream_area = upstream_area.isel(\n",
    "    latitude=slice(lat_slice_index[0], lat_slice_index[1]+1),\n",
    "    longitude=slice(lon_slice_index[0], lon_slice_index[1]+1),\n",
    ")\n",
    "\n",
    "# There are very minor rounding differences, so we update with the lat/lons from the glofas data\n",
    "red_upstream_area = red_upstream_area.assign_coords({\n",
    "    'latitude': glofas_data.latitude,\n",
    "    'longitude': glofas_data.longitude,\n",
    "})\n",
    "\n",
    "# Add the upstream area to the main data object and print the updated glofas data object:\n",
    "glofas_data['uparea'] = red_upstream_area['uparea']\n",
    "glofas_data\n",
    "\n",
    "# Mask the river discharge data\n",
    "glofas_data_masked = glofas_data.where(glofas_data.uparea>=area_filter*1e6)\n",
    "\n",
    "# Load the basin outlet data\n",
    "basin_outlet_file = r\"D:\\projects\\sovereign-risk\\Thailand\\data\\flood\\dependence\\thailand-basins\\lev06_outlets_final_clipped_Thailand_no_duplicates.csv\"\n",
    "basin_outlet_df = pd.read_csv(basin_outlet_file)\n",
    "# Note to align the two datasets we need to make the following adjustment to lat lons (based on previous trial and error)\n",
    "basin_outlet_df['Latitude'] = basin_outlet_df['Latitude'] + 0.05/2\n",
    "basin_outlet_df['Longitude'] = basin_outlet_df['Longitude'] - 0.05/2\n",
    "\n",
    "# Define function for checking timeseries\n",
    "def check_timeseries(array, latitude, longitude):\n",
    "    test_point = array.sel(latitude=latitude, longitude=longitude, method='nearest')\n",
    "    test_timeseries = test_point['dis24']\n",
    "    test_acc = float(test_point['uparea'])\n",
    "    # check for NaN values\n",
    "    non_nan_count = test_timeseries.count().item()\n",
    "    total_count = test_timeseries.size\n",
    "    nan_ratio = non_nan_count/total_count\n",
    "\n",
    "    # Does the timeseries pass the NaN threshold\n",
    "    if nan_ratio < 1:\n",
    "        return False, test_acc, \"NaN values found\"\n",
    "\n",
    "    # Check for constant values\n",
    "    if test_timeseries.min() == test_timeseries.max():\n",
    "        return False, test_acc, \"Constant timeseries values\"\n",
    "\n",
    "    # If all checks pass\n",
    "    return True, test_acc, \"Valid timeseries\"\n",
    "\n",
    "# Loop through basins and check whether timeseries is valid\n",
    "results = []\n",
    "for index, row in basin_outlet_df.iterrows():\n",
    "    latitude = row['Latitude']\n",
    "    longitude = row['Longitude']\n",
    "\n",
    "    valid, acc, message = check_timeseries(glofas_data_masked, latitude, longitude)\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'HYBAS_ID': row['HYBAS_ID'],\n",
    "        'Latitude': latitude,\n",
    "        'Longitude': longitude,\n",
    "        'Acc': acc,\n",
    "        'Valid': valid,\n",
    "        'Message': message\n",
    "    })\n",
    "    if not valid:\n",
    "        print(f\"ID: {row['HYBAS_ID']}, Lat: {latitude}, Lon: {longitude}, Acc: {acc}, Valid: {valid}, Message: {message}\")\n",
    "\n",
    "# over what years do we want to extract the data?\n",
    "start_year = 1979\n",
    "end_year = 2016\n",
    "sliced_data = glofas_data_masked.sel(time=slice(str(start_year), str(end_year)))\n",
    "# Dictionary to store timeseries data for each basin\n",
    "basin_timeseries = {}\n",
    "\n",
    "# Loop through basin outlets, storing each in turn\n",
    "for index, row in basin_outlet_df.iterrows():\n",
    "    basin_id = row['HYBAS_ID']\n",
    "    lat = row['Latitude']\n",
    "    lon = row['Longitude']\n",
    "    point_data = sliced_data.sel(latitude=lat, longitude=lon, method='nearest')\n",
    "    timeseries = point_data['dis24'].to_series()\n",
    "    # store in dictionary\n",
    "    basin_timeseries[basin_id] = timeseries\n",
    "\n",
    "# Dictionary to store fitted parameters for each basin\n",
    "gumbel_params = {}\n",
    "fit_quality = {}\n",
    "\n",
    "# Loop through basins, calculating annual maxima and fitting Gumbel distribution using L-moments\n",
    "for basin_id, timeseries in basin_timeseries.items():\n",
    "    annual_maxima = timeseries.groupby(timeseries.index.year).max()\n",
    "\n",
    "    # Fit Gumbel distribution using L-moments\n",
    "    params = distr.gum.lmom_fit(annual_maxima)\n",
    "\n",
    "    # Perform the Kolmogorov-Smirnov test (checking quality of fit)\n",
    "    D, p_value = kstest(annual_maxima, 'gumbel_r', args=(params['loc'], params['scale']))\n",
    "\n",
    "    gumbel_params[basin_id] = params\n",
    "    fit_quality[basin_id] = (D, p_value)\n",
    "\n",
    "# Will do this using the CDF of the fitted Gumbel distribution \n",
    "\n",
    "# Dictionary to story uniform marginals for each basin\n",
    "uniform_marginals = {}\n",
    "\n",
    "for basin_id, timeseries in basin_timeseries.items():\n",
    "    annual_maxima = timeseries.groupby(timeseries.index.year).max()\n",
    "    params = gumbel_params[basin_id]\n",
    "    uniform_marginals[basin_id] = gumbel_r.cdf(annual_maxima, loc=params['loc'], scale=params['scale'])\n",
    "\n",
    "# Dictionary to store copula models for each pair of basins\n",
    "clayton_copula_models = {}\n",
    "clayton_error_basins = [] # list to store basins that cause an error\n",
    "\n",
    "for id1, margins1 in uniform_marginals.items():\n",
    "    for id2, margins2 in uniform_marginals.items():\n",
    "        if id1 < id2: # to avoid duplicate pairs\n",
    "            try:\n",
    "                # Prepare the data for copula\n",
    "                data = np.column_stack((1-margins1, 1-margins2)) # interested in upper tail dependence so take inverse of CDF\n",
    "                \n",
    "                # Fit the Clayton copula\n",
    "                flipped_clayton = Clayton()\n",
    "                flipped_clayton.fit(data)\n",
    "    \n",
    "                # Store the copula model\n",
    "                clayton_copula_models[(id1, id2)] = flipped_clayton\n",
    "            except ValueError as e:\n",
    "                # print(f\"Error fitting Clayton copula for basins {id1} and {id2}: {e}\")\n",
    "                clayton_error_basins.append((id1, id2))\n",
    "\n",
    "# Store these copula pairs in a matrix\n",
    "\n",
    "basin_ids = list(uniform_marginals.keys()) # take the basin IDs from the uniform marginals dictionary\n",
    "N = len(basin_ids)\n",
    "\n",
    "\n",
    "# Initialize the matrix with NaNs\n",
    "dependence_matrix = np.full((N, N), np.nan)\n",
    "\n",
    "# Map from basin ID to matrix index\n",
    "id_to_index = {basin_id: index for index, basin_id in enumerate(basin_ids)}\n",
    "\n",
    "for (id1, id2), copula_model in clayton_copula_models.items():\n",
    "    index1, index2 = id_to_index[id1], id_to_index[id2]\n",
    "    dependence_matrix[index1, index2] = copula_model.theta\n",
    "    dependence_matrix[index2, index1] = copula_model.theta\n",
    "\n",
    "# For error basins do the same but set theta to -1\n",
    "for (id1, id2) in clayton_error_basins:\n",
    "    index1, index2 = id_to_index[id1], id_to_index[id2]\n",
    "    dependence_matrix[index1, index2] = -1\n",
    "    dependence_matrix[index2, index1] = -1\n",
    "\n",
    "# Debug (for infinity values) - not sure if needed but there are a few where I had to reassign basin outlets.\n",
    "dependence_matrix[np.isinf(dependence_matrix)] = 1000\n",
    "\n",
    "# Step 1: Find the most dependent pair\n",
    "# Initialize a set to keep track of selected basin indices\n",
    "selected_indices = set()\n",
    "# convert dependence_matrix to a masked array, so that NaN values and -1 are not considered in the operation\n",
    "masked_dependence_matrix = np.ma.masked_less(dependence_matrix, 0) # masking out values < 0\n",
    "np.fill_diagonal(masked_dependence_matrix, np.ma.masked) # we want to ignore diagonal (NaN values)\n",
    "max_theta_index = np.unravel_index(np.argmax(masked_dependence_matrix, axis=None), masked_dependence_matrix.shape)\n",
    "ordered_basins = [basin_ids[max_theta_index[0]], basin_ids[max_theta_index[1]]]\n",
    "# Add indices to the set of selected indices\n",
    "selected_indices.update([max_theta_index[0], max_theta_index[1]])\n",
    "\n",
    "# Step 2-4: Loop until all basins are ordered\n",
    "while len(ordered_basins) < len(basin_ids):\n",
    "    # Step 2: Choose basin k that is dependent on both basin i, j (last two basins in ordered_basins). Minimax approach\n",
    "    # Exclude already selected basins from the selection process\n",
    "    potential_next_indices = [i for i in range(len(basin_ids)) if i not in selected_indices]\n",
    "    # Find the indices of the last two basins in ordered_basins\n",
    "    last_two_indices = [id_to_index[basin] for basin in ordered_basins[-2:]]\n",
    "    # Find dependency vectors for the last two basins\n",
    "    dependency_vectors = masked_dependence_matrix[last_two_indices, :]\n",
    "    # Calculate the minimum dependency for each row of the vector\n",
    "    min_deps = np.ma.min(dependency_vectors, axis=0)\n",
    "    # Mask already selected indices\n",
    "    min_deps_masked = np.ma.copy(min_deps)\n",
    "    # Debug \n",
    "    # Ensure min_deps_masked.mask is an array\n",
    "    if np.isscalar(min_deps_masked.mask):\n",
    "        min_deps_masked.mask = np.zeros(min_deps_masked.shape, dtype=bool)\n",
    "    for idx in selected_indices:\n",
    "        min_deps_masked.mask[idx] = True # mask the index if it's already in selected indices\n",
    "    # Step 3: Find the maximum dependency value over the minimized vector - which will be the next basin\n",
    "    next_basin_index = np.ma.argmax(min_deps_masked, fill_value=-np.inf)\n",
    "    # Step 4: Continue iterations until there are no more basins left to process\n",
    "    # Check if all options are effectively masked\n",
    "    if min_deps_masked.mask.all():\n",
    "        print(\"No suitable next basin found. Ending process.\")\n",
    "        break\n",
    "    next_basin = basin_ids[next_basin_index]\n",
    "    ordered_basins.append(next_basin)\n",
    "    selected_indices.add(next_basin_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "542b9974-056a-4616-83aa-a5caba6a750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for risk analysis\n",
    "def generate_conditional_sample(v, theta, r):\n",
    "    '''\n",
    "    Generate a conditional sample using the Flipped Calyton copula.\n",
    "    Equation 12 from the Timonina et al (2015) paper\n",
    "\n",
    "    :param v: Known loss in basin i\n",
    "    :param theta: Copula parameter for dependency between basins i and j.\n",
    "    :param r: Random value from uniform distribution for sampling.\n",
    "    :retrun: Generated conditional loss in basin j.\n",
    "    '''\n",
    "    u = 1-(1+((1-v)**(-theta))*(((r**(-((theta)/(1+theta))))-1)))**(-(1/theta))\n",
    "    return u \n",
    "\n",
    "# # Function for extracting loss-probability curve for basin\n",
    "# def basin_loss_curve(loss_df, basin_id, basin_col, epoch_val, scenario_val, urban_class, rps):\n",
    "#     losses = [] # initialize empty list to store losses\n",
    "#     basin_df = loss_df[(loss_df[basin_col]==basin_id) & (loss_df['epoch']==epoch_val) & (loss_df['adaptation_scenario']==scenario_val) & (loss_df['urban_class']==urban_class)]\n",
    "#     grouped_basin_df = basin_df.groupby([basin_col, 'RP']).agg({'damages':'sum'}).reset_index()\n",
    "#     for i in rps:\n",
    "#         losses.append(grouped_basin_df.loc[grouped_basin_df['RP'] == i, 'damages'].sum())\n",
    "#     # Return losses (index indicates what the RP is)\n",
    "#     return losses\n",
    "\n",
    "def interpolate_damages(RPs, losses, sim_aep, protection_level=0.5):\n",
    "    aeps = [1/i for i in RPs]\n",
    "    # Ensure AEPs are in ascending order for np.interp\n",
    "    aeps.sort() \n",
    "    losses = losses[::-1]\n",
    "\n",
    "    # test\n",
    "    # sim_aep = 1 - sim_aep # convert extreme simulated AEPs (e.g. 0.95) to equivalent AEPs for interpolation (e.g. 0.05)\n",
    "\n",
    "    # Interpolate based off simulated AEP\n",
    "    if sim_aep >= protection_level: \n",
    "        return 0 \n",
    "    else:\n",
    "        interpolated_value = np.interp(sim_aep, aeps, losses)\n",
    "        return interpolated_value\n",
    "\n",
    "def get_copula_model(copula_models, basin1, basin2):\n",
    "    \"\"\"\n",
    "    Attempt to retrieve a copula model for a given pair of basins.\n",
    "    Tries both possible orders of the basin IDs.\n",
    "\n",
    "    :param copula_models: Dictionary of copula models.\n",
    "    :param basin1: ID of the first basin.\n",
    "    :param basin2: ID of the second basin.\n",
    "    :return: The copula model if found, else None.\n",
    "    \"\"\"\n",
    "    return copula_models.get((basin1, basin2)) or copula_models.get((basin2, basin1))\n",
    "\n",
    "# Function for Monte Carlo simulating incorporating basin dependencies\n",
    "def monte_carlo_dependence_simulation(loss_df, rps, basin_col, epoch_val, scenario_val, urban_class, protection_level, num_years, ordered_basins, copula_models, gumbel_params, num_simulations=10000):\n",
    "    '''\n",
    "    Perform Monte Carlo simulations of yearly losses incorporating basin dependencies.\n",
    "\n",
    "    :param loss_df: dataframe with losses from risk analysis\n",
    "    :param rps: list of return periods to consider. \n",
    "    :param basin_col: name of column for basins (e.g. 'HB_L6')\n",
    "    :param epoch_val: name of epoch value (e.g. 'Today')\n",
    "    :param scenario_val: name of scenario (e.g. 'Baseline')\n",
    "    :param urban_class: name of urban class to consider (e.g. 'Residential')\n",
    "    :param protection_level: what is the baseline protection level (e.g. 0.5 or 1 in 2 years)\n",
    "    :param num_years: Number of years to simulate\n",
    "    :param ordered_basins: List of basin IDs ordered by dependency\n",
    "    :param copula_models: Dictionary holding copula model for each basin pair\n",
    "    :param gumbel_params: Gumbel distribution parameters for each basin.\n",
    "    :param num__simulations: Number of simulations (default is 10,000).\n",
    "    :return: Dataframe of simulated national losses for each year.\n",
    "    '''\n",
    "\n",
    "    # To speed up the Monte-Carlo simulation we are going to pre-compute some variables\n",
    "    # precompute loss-probability curves for each basin\n",
    "    basin_loss_curves = {basin_id: basin_loss_curve(loss_df, basin_id, basin_col, epoch_val, scenario_val, urban_class, rps) for basin_id in ordered_basins}\n",
    "    # # Debug\n",
    "    # print(basin_loss_curves)\n",
    "    # Debug: we need to treat infiniti theta values (they are affecting the analysis) will replace these with v. high theta (1000)\n",
    "    # Initialize array for national losses\n",
    "    national_losses_per_year = np.zeros((num_simulations, num_years))\n",
    "    # Generate all random numbers in advance\n",
    "    random_numbers = np.random.uniform(0, 1, (num_simulations, num_years, len(ordered_basins))).astype(np.float32)\n",
    "\n",
    "    for simulation in range(num_simulations):\n",
    "        # print progress\n",
    "        if simulation % 50 == 0:\n",
    "            print('Simulation progress: %s out of %s' % (simulation, num_simulations))\n",
    "        for year in range(num_years):\n",
    "            # Initialize a list to store losses for each basin for the current year\n",
    "            yearly_losses = []\n",
    "            yearly_loss_values = []\n",
    "            for i, basin_id in enumerate(ordered_basins):\n",
    "                # print(basin_id)\n",
    "                if i == 0:\n",
    "                    # Handle first basin\n",
    "                    r = random_numbers[simulation, year, i]\n",
    "                    loss_curves = basin_loss_curves[basin_id]\n",
    "                    basin_loss = 0\n",
    "                    yearly_losses.append(r) # add current loss simulation to the list\n",
    "                    for Pr_L in loss_curves: # loop through basin protection levels\n",
    "                        if Pr_L <= r:\n",
    "                            # print(Pr_L, 'smaller than', r, 'continuing...') # if baseline protection is achieved...\n",
    "                            continue\n",
    "                        else:\n",
    "                            yearly_loss_values.append(interpolate_damages(rps, loss_curves[Pr_L], r, protection_level))\n",
    "                            \n",
    "                else:\n",
    "                    loss_curves = basin_loss_curves[basin_id]\n",
    "                    # Handle subsequent basins with dependencies\n",
    "                    copula = get_copula_model(copula_models, ordered_basins[i-1], basin_id)\n",
    "                    if copula is not None:\n",
    "                        # Apply dependency model if theta exists\n",
    "                        r = random_numbers[simulation, year, i]\n",
    "                        previous_loss = yearly_losses[i-1]\n",
    "                        current_loss = generate_conditional_sample(previous_loss, copula.theta, r)\n",
    "                        yearly_losses.append(current_loss)\n",
    "                        # in the below interpolation the (1-current_loss) part of the equation is critical.\n",
    "                        # because the copula is optimized to model tail dependencies (e.g. > 0.9) and our AEPs are \n",
    "                        # essentially inverted (e.g. 0.001 is extreme) we need to invert the random number for interpolating the\n",
    "                        # losses. This changes nothing apart from ensuring tail dependency is preserved. \n",
    "                        for Pr_L in loss_curves: # loop through basin protection levels\n",
    "                            if Pr_L <= current_loss:\n",
    "                                # print(Pr_L, 'smaller than', r, 'continuing...') # if baseline protection is achieved...\n",
    "                                continue\n",
    "                            else:\n",
    "                                yearly_loss_values.append(interpolate_damages(rps, loss_curves[Pr_L], current_loss, protection_level))\n",
    "                        # yearly_loss_values.append(interpolate_damages(rps, loss_curve, current_loss))# 1-current_loss)) # test\n",
    "                        # yearly_loss_values.append(interpolate_damages(rps, loss_curve, (1-current_loss)))# 1-current_loss)) # test if this is valid\n",
    "                        # distribution_figure(copula.theta, r, previous_loss, (current_loss))\n",
    "                    else:\n",
    "                        # Independent simulation for this basin\n",
    "                        r = random_numbers[simulation, year, i]\n",
    "                        yearly_losses.append(r)\n",
    "                        for Pr_L in loss_curves: # loop through basin protection levels\n",
    "                            if Pr_L <= r:\n",
    "                                # print(Pr_L, 'smaller than', r, 'continuing...') # if baseline protection is achieved...\n",
    "                                continue\n",
    "                            else:\n",
    "                                yearly_loss_values.append(interpolate_damages(rps, loss_curves[Pr_L], r, protection_level))\n",
    "\n",
    "            # Aggregate losses for the current year\n",
    "            national_losses_per_year[simulation, year] = sum(yearly_loss_values)\n",
    "\n",
    "    # Convert the results into a DataFrame\n",
    "    return pd.DataFrame(national_losses_per_year, columns=[f'Year_{i+1}' for i in range(num_years)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a22bce58-bd7e-4e7d-b3f8-7093d41b55f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Monte Carlo simulating incorporating basin dependencies\n",
    "def urban_monte_carlo_dependence_simulation(loss_df, rps, basin_col, epoch_val, scenario_val, urban_class, protection_level, num_years, ordered_basins, copula_models, gumbel_params, num_simulations=10000):\n",
    "    '''\n",
    "    Adjusted to account for urban protection\n",
    "    Perform Monte Carlo simulations of yearly losses incorporating basin dependencies. This function is specifically for simulating urban flood protection\n",
    "\n",
    "    :param loss_df: dataframe with losses from risk analysis\n",
    "    :param rps: list of return periods to consider. \n",
    "    :param basin_col: name of column for basins (e.g. 'HB_L6')\n",
    "    :param epoch_val: name of epoch value (e.g. 'Today')\n",
    "    :param scenario_val: name of scenario (e.g. 'Baseline')\n",
    "    :param urban_class: name of urban class to consider (e.g. 'Residential')\n",
    "    :param protection_level: what is the baseline protection level (e.g. 0.5 or 1 in 2 years)\n",
    "    :param num_years: Number of years to simulate\n",
    "    :param ordered_basins: List of basin IDs ordered by dependency\n",
    "    :param copula_models: Dictionary holding copula model for each basin pair\n",
    "    :param gumbel_params: Gumbel distribution parameters for each basin.\n",
    "    :param num__simulations: Number of simulations (default is 10,000).\n",
    "    :return: Dataframe of simulated national losses for each year.\n",
    "    '''\n",
    "\n",
    "    # To speed up the Monte-Carlo simulation we are going to pre-compute some variables\n",
    "    # precompute loss-probability curves for each basin\n",
    "    urban_basin_loss_curves = {basin_id: basin_loss_curve(loss_df, basin_id, basin_col, epoch_val, scenario_val, urban_class, rps) for basin_id in ordered_basins}\n",
    "    basin_loss_curves = {basin_id: basin_loss_curve(loss_df, basin_id, basin_col, epoch_val, 'Baseline', urban_class, rps) for basin_id in ordered_basins}\n",
    "    # # Debug\n",
    "    # print(basin_loss_curves)\n",
    "    # Debug: we need to treat infiniti theta values (they are affecting the analysis) will replace these with v. high theta (1000)\n",
    "    # Initialize array for national losses\n",
    "    national_losses_per_year = np.zeros((num_simulations, num_years))\n",
    "    # Generate all random numbers in advance\n",
    "    random_numbers = np.random.uniform(0, 1, (num_simulations, num_years, len(ordered_basins))).astype(np.float32)\n",
    "\n",
    "    for simulation in range(num_simulations):\n",
    "        # print progress\n",
    "        if simulation % 50 == 0:\n",
    "            print('Simulation progress: %s out of %s' % (simulation, num_simulations))\n",
    "        for year in range(num_years):\n",
    "            # Initialize a list to store losses for each basin for the current year\n",
    "            yearly_losses = []\n",
    "            yearly_loss_values = []\n",
    "            for i, basin_id in enumerate(ordered_basins):\n",
    "                # print(basin_id)\n",
    "                if i == 0:\n",
    "                    # Handle first basin\n",
    "                    r = random_numbers[simulation, year, i]\n",
    "                    urban_loss_curves = urban_basin_loss_curves[basin_id]\n",
    "                    loss_curves = basin_loss_curves[basin_id]\n",
    "                    yearly_losses.append(r) # add current loss simulation to the list\n",
    "                    if r < 0.01: # use baseline maps if AEP < 0.01\n",
    "                        for Pr_L in loss_curves:\n",
    "                            yearly_loss_values.append(interpolate_damages(rps, loss_curves[Pr_L], r, protection_level))\n",
    "                    else:\n",
    "                        for Pr_L in urban_loss_curves:\n",
    "                            if Pr_L <= r:\n",
    "                                continue\n",
    "                            else:\n",
    "                                yearly_loss_values.append(interpolate_damages(rps, urban_loss_curves[Pr_L], r, protection_level))\n",
    "                            \n",
    "                else:\n",
    "                    urban_loss_curves = urban_basin_loss_curves[basin_id]\n",
    "                    loss_curves = basin_loss_curves[basin_id]\n",
    "                    # Handle subsequent basins with dependencies\n",
    "                    copula = get_copula_model(copula_models, ordered_basins[i-1], basin_id)\n",
    "                    if copula is not None:\n",
    "                        # Apply dependency model if theta exists\n",
    "                        r = random_numbers[simulation, year, i]\n",
    "                        previous_loss = yearly_losses[i-1]\n",
    "                        current_loss = generate_conditional_sample(previous_loss, copula.theta, r)\n",
    "                        yearly_losses.append(current_loss)\n",
    "                        # in the below interpolation the (1-current_loss) part of the equation is critical.\n",
    "                        # because the copula is optimized to model tail dependencies (e.g. > 0.9) and our AEPs are \n",
    "                        # essentially inverted (e.g. 0.001 is extreme) we need to invert the random number for interpolating the\n",
    "                        # losses. This changes nothing apart from ensuring tail dependency is preserved. \n",
    "                        if current_loss < 0.01: # Use baseline maps\n",
    "                            for Pr_L in loss_curves: \n",
    "                                yearly_loss_values.append(interpolate_damages(rps, loss_curves[Pr_L], current_loss, protection_level))\n",
    "                        else:\n",
    "                            for Pr_L in urban_loss_curves:\n",
    "                                if Pr_L <= current_loss:\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    yearly_loss_values.append(interpolate_damages(rps, urban_loss_curves[Pr_L], current_loss, protection_level))\n",
    "                        # yearly_loss_values.append(interpolate_damages(rps, loss_curve, current_loss))# 1-current_loss)) # test\n",
    "                        # yearly_loss_values.append(interpolate_damages(rps, loss_curve, (1-current_loss)))# 1-current_loss)) # test if this is valid\n",
    "                        # distribution_figure(copula.theta, r, previous_loss, (current_loss))\n",
    "                    else:\n",
    "                        # Independent simulation for this basin\n",
    "                        r = random_numbers[simulation, year, i]\n",
    "                        yearly_losses.append(r)\n",
    "                        if r < 0.01: # use baseline maps if AEP < 0.01\n",
    "                            for Pr_L in loss_curves:\n",
    "                                yearly_loss_values.append(interpolate_damages(rps, loss_curves[Pr_L], r, protection_level))\n",
    "                        else:\n",
    "                            for Pr_L in urban_loss_curves:\n",
    "                                if Pr_L <= r:\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    yearly_loss_values.append(interpolate_damages(rps, urban_loss_curves[Pr_L], r, protection_level))\n",
    "\n",
    "            # Aggregate losses for the current year\n",
    "            national_losses_per_year[simulation, year] = sum(yearly_loss_values)\n",
    "\n",
    "    # Convert the results into a DataFrame\n",
    "    return pd.DataFrame(national_losses_per_year, columns=[f'Year_{i+1}' for i in range(num_years)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82a815d7-1271-4a3d-a779-07720f0dcde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: adjusting function to account for existing levels of protection\n",
    "def basin_loss_curve(loss_df, basin_id, basin_col, epoch_val, scenario_val, urban_class, rps):\n",
    "    losses = {} # initialize empty dictionary to store losses and protection level\n",
    "    basin_df = loss_df[(loss_df[basin_col]==basin_id) & (loss_df['epoch']==epoch_val) & (loss_df['adaptation_scenario']==scenario_val) & (loss_df['urban_class']==urban_class)]\n",
    "    grouped_basin_df = basin_df.groupby([basin_col, 'RP', 'Pr_L_AEP']).agg({'damages':'sum'}).reset_index()\n",
    "    # # Pull unique protection levels from the grouped dataframe\n",
    "    unique_protection_levels = grouped_basin_df['Pr_L_AEP'].unique()\n",
    "    if len(unique_protection_levels) == 0:\n",
    "        unique_protection_levels = [1]\n",
    "    for i in unique_protection_levels:\n",
    "        losses[i] = [grouped_basin_df.loc[(grouped_basin_df['RP'] == rp) & (grouped_basin_df['Pr_L_AEP']==i), 'damages'].sum() for rp in rps]\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27bbab65-7492-4a54-8f16-cc064c5da640",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'risk_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m rps \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m1000\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m test_losses \u001b[38;5;241m=\u001b[39m monte_carlo_dependence_simulation(\u001b[43mrisk_data\u001b[49m, rps, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHB_L6\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mToday\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBaseline\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCombined\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1\u001b[39m, ordered_basins, clayton_copula_models, gumbel_params, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'risk_data' is not defined"
     ]
    }
   ],
   "source": [
    "rps = [2, 5, 10, 25, 50, 100, 200, 500, 1000]\n",
    "test_losses = monte_carlo_dependence_simulation(risk_data, rps, 'HB_L6', 'Today', 'Baseline', 'Combined', 0.5, 1, ordered_basins, clayton_copula_models, gumbel_params, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d43b5284-b984-4f56-8680-1eab169f21a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>FID</th>\n",
       "      <th>GID_1</th>\n",
       "      <th>NAME</th>\n",
       "      <th>HB_L4</th>\n",
       "      <th>HB_L5</th>\n",
       "      <th>HB_L6</th>\n",
       "      <th>HB_L7</th>\n",
       "      <th>Pr_L</th>\n",
       "      <th>Add_Pr</th>\n",
       "      <th>New_Pr_L</th>\n",
       "      <th>damages</th>\n",
       "      <th>epoch</th>\n",
       "      <th>adaptation_scenario</th>\n",
       "      <th>RP</th>\n",
       "      <th>urban_class</th>\n",
       "      <th>AEP</th>\n",
       "      <th>Pr_L_AEP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12438.0</td>\n",
       "      <td>0</td>\n",
       "      <td>THA.62_1</td>\n",
       "      <td>Si Sa Ket</td>\n",
       "      <td>4.041145e+09</td>\n",
       "      <td>4.051145e+09</td>\n",
       "      <td>4.061140e+09</td>\n",
       "      <td>4.071125e+09</td>\n",
       "      <td>16.387501</td>\n",
       "      <td>83.612499</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Future_High_Emission</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>2</td>\n",
       "      <td>Residential</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.061022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18657.0</td>\n",
       "      <td>0</td>\n",
       "      <td>THA.62_1</td>\n",
       "      <td>Si Sa Ket</td>\n",
       "      <td>4.041145e+09</td>\n",
       "      <td>4.051145e+09</td>\n",
       "      <td>4.061140e+09</td>\n",
       "      <td>4.071125e+09</td>\n",
       "      <td>16.387501</td>\n",
       "      <td>83.612499</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Future_High_Emission</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>2</td>\n",
       "      <td>Non-Residential</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.061022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>THA.62_1</td>\n",
       "      <td>Si Sa Ket</td>\n",
       "      <td>4.041145e+09</td>\n",
       "      <td>4.051145e+09</td>\n",
       "      <td>4.061140e+09</td>\n",
       "      <td>4.071125e+09</td>\n",
       "      <td>16.387501</td>\n",
       "      <td>83.612499</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Future_High_Emission</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>2</td>\n",
       "      <td>Combined</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.061022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13129.0</td>\n",
       "      <td>0</td>\n",
       "      <td>THA.62_1</td>\n",
       "      <td>Si Sa Ket</td>\n",
       "      <td>4.041145e+09</td>\n",
       "      <td>4.051145e+09</td>\n",
       "      <td>4.061140e+09</td>\n",
       "      <td>4.071125e+09</td>\n",
       "      <td>16.387501</td>\n",
       "      <td>83.612499</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Future_High_Emission</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>5</td>\n",
       "      <td>Residential</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.061022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19348.0</td>\n",
       "      <td>0</td>\n",
       "      <td>THA.62_1</td>\n",
       "      <td>Si Sa Ket</td>\n",
       "      <td>4.041145e+09</td>\n",
       "      <td>4.051145e+09</td>\n",
       "      <td>4.061140e+09</td>\n",
       "      <td>4.071125e+09</td>\n",
       "      <td>16.387501</td>\n",
       "      <td>83.612499</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Future_High_Emission</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>5</td>\n",
       "      <td>Non-Residential</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.061022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  FID     GID_1       NAME         HB_L4         HB_L5  \\\n",
       "0     12438.0    0  THA.62_1  Si Sa Ket  4.041145e+09  4.051145e+09   \n",
       "1     18657.0    0  THA.62_1  Si Sa Ket  4.041145e+09  4.051145e+09   \n",
       "2         NaN    0  THA.62_1  Si Sa Ket  4.041145e+09  4.051145e+09   \n",
       "3     13129.0    0  THA.62_1  Si Sa Ket  4.041145e+09  4.051145e+09   \n",
       "4     19348.0    0  THA.62_1  Si Sa Ket  4.041145e+09  4.051145e+09   \n",
       "\n",
       "          HB_L6         HB_L7       Pr_L     Add_Pr  New_Pr_L  damages  \\\n",
       "0  4.061140e+09  4.071125e+09  16.387501  83.612499     100.0      0.0   \n",
       "1  4.061140e+09  4.071125e+09  16.387501  83.612499     100.0      0.0   \n",
       "2  4.061140e+09  4.071125e+09  16.387501  83.612499     100.0      0.0   \n",
       "3  4.061140e+09  4.071125e+09  16.387501  83.612499     100.0      0.0   \n",
       "4  4.061140e+09  4.071125e+09  16.387501  83.612499     100.0      0.0   \n",
       "\n",
       "                  epoch adaptation_scenario  RP      urban_class  AEP  \\\n",
       "0  Future_High_Emission            Baseline   2      Residential  0.5   \n",
       "1  Future_High_Emission            Baseline   2  Non-Residential  0.5   \n",
       "2  Future_High_Emission            Baseline   2         Combined  0.5   \n",
       "3  Future_High_Emission            Baseline   5      Residential  0.2   \n",
       "4  Future_High_Emission            Baseline   5  Non-Residential  0.2   \n",
       "\n",
       "   Pr_L_AEP  \n",
       "0  0.061022  \n",
       "1  0.061022  \n",
       "2  0.061022  \n",
       "3  0.061022  \n",
       "4  0.061022  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load risk data\n",
    "risk_data = pd.read_csv(r\"D:\\projects\\sovereign-risk\\Thailand\\analysis\\flood\\risk_analysis\\risk_basin_zonal_sum.csv\")\n",
    "# Add columne for annual exceedance probability\n",
    "risk_data['AEP'] = 1 / risk_data['RP']\n",
    "# Add a column converting current prorection level into AEP\n",
    "risk_data['Pr_L_AEP'] = np.where(risk_data['Pr_L'] == 0, 0, 1 / risk_data['Pr_L']) # using numpy where avoids zero division errors\n",
    "#### Add row for each combination that sums residential and non-residential damages\n",
    "grouped = risk_data.groupby(['FID', 'GID_1', 'NAME', 'HB_L4', 'HB_L5', 'HB_L6', 'HB_L7', 'Pr_L', 'Pr_L_AEP', 'Add_Pr', 'New_Pr_L', 'epoch', 'adaptation_scenario', 'RP', 'AEP'], as_index=False)['damages'].sum()\n",
    "grouped['urban_class'] = 'Combined'  # Add a column for urban_class with value 'total'\n",
    "risk_data = pd.concat([risk_data, grouped], ignore_index=True).sort_values(by=['FID', 'GID_1', 'NAME', 'HB_L4', 'HB_L5', 'HB_L6', 'HB_L7', 'Pr_L', 'Pr_L_AEP', 'Add_Pr', 'New_Pr_L', 'epoch', 'adaptation_scenario', 'RP', 'AEP'])\n",
    "risk_data.reset_index(drop=True, inplace=True)\n",
    "risk_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca24cb47-4f76-4b05-b4cf-b512419fa114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation progress: 0 out of 1000\n",
      "Simulation progress: 50 out of 1000\n",
      "Simulation progress: 100 out of 1000\n",
      "Simulation progress: 150 out of 1000\n",
      "Simulation progress: 200 out of 1000\n",
      "Simulation progress: 250 out of 1000\n",
      "Simulation progress: 300 out of 1000\n",
      "Simulation progress: 350 out of 1000\n",
      "Simulation progress: 400 out of 1000\n",
      "Simulation progress: 450 out of 1000\n",
      "Simulation progress: 500 out of 1000\n",
      "Simulation progress: 550 out of 1000\n",
      "Simulation progress: 600 out of 1000\n",
      "Simulation progress: 650 out of 1000\n",
      "Simulation progress: 700 out of 1000\n",
      "Simulation progress: 750 out of 1000\n",
      "Simulation progress: 800 out of 1000\n",
      "Simulation progress: 850 out of 1000\n",
      "Simulation progress: 900 out of 1000\n",
      "Simulation progress: 950 out of 1000\n"
     ]
    }
   ],
   "source": [
    "baseline_losses = monte_carlo_dependence_simulation(risk_data, rps, 'HB_L6', 'Today', 'Baseline', 'Combined', 0.5, 500, ordered_basins, clayton_copula_models, gumbel_params, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afb41cea-b2cf-4614-9f3e-f123ffc81e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation progress: 0 out of 1000\n",
      "Simulation progress: 50 out of 1000\n",
      "Simulation progress: 100 out of 1000\n",
      "Simulation progress: 150 out of 1000\n",
      "Simulation progress: 200 out of 1000\n",
      "Simulation progress: 250 out of 1000\n",
      "Simulation progress: 300 out of 1000\n",
      "Simulation progress: 350 out of 1000\n",
      "Simulation progress: 400 out of 1000\n",
      "Simulation progress: 450 out of 1000\n",
      "Simulation progress: 500 out of 1000\n",
      "Simulation progress: 550 out of 1000\n",
      "Simulation progress: 600 out of 1000\n",
      "Simulation progress: 650 out of 1000\n",
      "Simulation progress: 700 out of 1000\n",
      "Simulation progress: 750 out of 1000\n",
      "Simulation progress: 800 out of 1000\n",
      "Simulation progress: 850 out of 1000\n",
      "Simulation progress: 900 out of 1000\n",
      "Simulation progress: 950 out of 1000\n"
     ]
    }
   ],
   "source": [
    "high_emission_losses = monte_carlo_dependence_simulation(risk_data, rps, 'HB_L6', 'Future_High_Emission', 'Baseline', 'Combined', 0.5, 500, ordered_basins, clayton_copula_models, gumbel_params, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c925e38-e165-4955-92de-ee4254d3c36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation progress: 0 out of 1000\n",
      "Simulation progress: 50 out of 1000\n",
      "Simulation progress: 100 out of 1000\n",
      "Simulation progress: 150 out of 1000\n",
      "Simulation progress: 200 out of 1000\n",
      "Simulation progress: 250 out of 1000\n",
      "Simulation progress: 300 out of 1000\n",
      "Simulation progress: 350 out of 1000\n",
      "Simulation progress: 400 out of 1000\n",
      "Simulation progress: 450 out of 1000\n",
      "Simulation progress: 500 out of 1000\n",
      "Simulation progress: 550 out of 1000\n",
      "Simulation progress: 600 out of 1000\n",
      "Simulation progress: 650 out of 1000\n",
      "Simulation progress: 700 out of 1000\n",
      "Simulation progress: 750 out of 1000\n",
      "Simulation progress: 800 out of 1000\n",
      "Simulation progress: 850 out of 1000\n",
      "Simulation progress: 900 out of 1000\n",
      "Simulation progress: 950 out of 1000\n"
     ]
    }
   ],
   "source": [
    "low_emission_losses = monte_carlo_dependence_simulation(risk_data, rps, 'HB_L6', 'Future_Low_Emission', 'Baseline', 'Combined', 0.5, 500, ordered_basins, clayton_copula_models, gumbel_params, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd488313-d42f-4f64-a9c3-c8bc985becc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation progress: 0 out of 1000\n",
      "Simulation progress: 50 out of 1000\n",
      "Simulation progress: 100 out of 1000\n",
      "Simulation progress: 150 out of 1000\n",
      "Simulation progress: 200 out of 1000\n",
      "Simulation progress: 250 out of 1000\n",
      "Simulation progress: 300 out of 1000\n",
      "Simulation progress: 350 out of 1000\n",
      "Simulation progress: 400 out of 1000\n",
      "Simulation progress: 450 out of 1000\n",
      "Simulation progress: 500 out of 1000\n",
      "Simulation progress: 550 out of 1000\n",
      "Simulation progress: 600 out of 1000\n",
      "Simulation progress: 650 out of 1000\n",
      "Simulation progress: 700 out of 1000\n",
      "Simulation progress: 750 out of 1000\n",
      "Simulation progress: 800 out of 1000\n",
      "Simulation progress: 850 out of 1000\n",
      "Simulation progress: 900 out of 1000\n",
      "Simulation progress: 950 out of 1000\n"
     ]
    }
   ],
   "source": [
    "urban_protection_high_emission = urban_monte_carlo_dependence_simulation(risk_data, rps, 'HB_L6', 'Future_High_Emission', 'Urban_Protection_RP100', 'Combined', 0.5, 500, ordered_basins, clayton_copula_models, gumbel_params, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1f54de1-3fc0-4208-b517-14bf48020e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation progress: 0 out of 1000\n",
      "Simulation progress: 50 out of 1000\n",
      "Simulation progress: 100 out of 1000\n",
      "Simulation progress: 150 out of 1000\n",
      "Simulation progress: 200 out of 1000\n",
      "Simulation progress: 250 out of 1000\n",
      "Simulation progress: 300 out of 1000\n",
      "Simulation progress: 350 out of 1000\n",
      "Simulation progress: 400 out of 1000\n",
      "Simulation progress: 450 out of 1000\n",
      "Simulation progress: 500 out of 1000\n",
      "Simulation progress: 550 out of 1000\n",
      "Simulation progress: 600 out of 1000\n",
      "Simulation progress: 650 out of 1000\n",
      "Simulation progress: 700 out of 1000\n",
      "Simulation progress: 750 out of 1000\n",
      "Simulation progress: 800 out of 1000\n",
      "Simulation progress: 850 out of 1000\n",
      "Simulation progress: 900 out of 1000\n",
      "Simulation progress: 950 out of 1000\n"
     ]
    }
   ],
   "source": [
    "dry_proofing_high_emission = monte_carlo_dependence_simulation(risk_data, rps, 'HB_L6', 'Future_High_Emission', 'Dry_Proofing', 'Combined', 0.5, 500, ordered_basins, clayton_copula_models, gumbel_params, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f782e99-7024-4c7d-96cd-b63db5adeb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation progress: 0 out of 1000\n",
      "Simulation progress: 50 out of 1000\n",
      "Simulation progress: 100 out of 1000\n",
      "Simulation progress: 150 out of 1000\n",
      "Simulation progress: 200 out of 1000\n",
      "Simulation progress: 250 out of 1000\n",
      "Simulation progress: 300 out of 1000\n",
      "Simulation progress: 350 out of 1000\n",
      "Simulation progress: 400 out of 1000\n",
      "Simulation progress: 450 out of 1000\n",
      "Simulation progress: 500 out of 1000\n",
      "Simulation progress: 550 out of 1000\n",
      "Simulation progress: 600 out of 1000\n",
      "Simulation progress: 650 out of 1000\n",
      "Simulation progress: 700 out of 1000\n",
      "Simulation progress: 750 out of 1000\n",
      "Simulation progress: 800 out of 1000\n",
      "Simulation progress: 850 out of 1000\n",
      "Simulation progress: 900 out of 1000\n",
      "Simulation progress: 950 out of 1000\n"
     ]
    }
   ],
   "source": [
    "relocation_high_emission = monte_carlo_dependence_simulation(risk_data, rps, 'HB_L6', 'Future_High_Emission', 'Relocation', 'Combined', 0.5, 500, ordered_basins, clayton_copula_models, gumbel_params, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e0afee8-f099-4326-bb59-59cea0d4478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss-probability curve\n",
    "baseline_all_losses = baseline_losses.values.flatten()  # Flatten to get a single array of losses\n",
    "baseline_sorted_losses = np.sort(baseline_all_losses)[::-1]  # Sort losses from highest to lowest\n",
    "low_emission_all_losses = low_emission_losses.values.flatten()  # Flatten to get a single array of losses\n",
    "low_emission_sorted_losses = np.sort(low_emission_all_losses)[::-1]  # Sort losses from highest to lowest\n",
    "high_emission_all_losses = high_emission_losses.values.flatten()  # Flatten to get a single array of losses\n",
    "high_emission_sorted_losses = np.sort(high_emission_all_losses)[::-1]  # Sort losses from highest to lowest\n",
    "urban_protection_high_emission_all_losses = urban_protection_high_emission.values.flatten()  # Flatten to get a single array of losses\n",
    "urban_protection_high_emission_sorted_losses = np.sort(urban_protection_high_emission_all_losses)[::-1]  # Sort losses from highest to lowest\n",
    "dry_proofing_high_emission_all_losses = dry_proofing_high_emission.values.flatten()  # Flatten to get a single array of losses\n",
    "dry_proofing_high_emission_sorted_losses = np.sort(dry_proofing_high_emission_all_losses)[::-1]  # Sort losses from highest to lowest\n",
    "relocation_high_emission_all_losses = relocation_high_emission.values.flatten()  # Flatten to get a single array of losses\n",
    "relocation_high_emission_sorted_losses = np.sort(relocation_high_emission_all_losses)[::-1]  # Sort losses from highest to lowest\n",
    "aeps = np.arange(1, len(baseline_sorted_losses) + 1) / len(baseline_sorted_losses)  # Calculate AEPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0429e7eb-735f-4454-9d22-672e9628c791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot loss-probability curve\n",
    "# all_losses = losses.values.flatten()  # Flatten to get a single array of losses\n",
    "# sorted_losses = np.sort(all_losses)[::-1]  # Sort losses from highest to lowest\n",
    "# aeps = np.arange(1, len(sorted_losses) + 1) / len(sorted_losses)  # Calculate AEPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "138dbc4a-d74f-4cde-9efc-56fee4147c14",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39msemilogx(aeps, baseline_sorted_losses, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, markersize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBaseline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39msemilogx(aeps, high_emission_sorted_losses, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, markersize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFuture High Emission\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(aeps, baseline_sorted_losses, marker='o', linestyle='-', markersize=2, label='Baseline')\n",
    "plt.semilogx(aeps, high_emission_sorted_losses, marker='o', linestyle='-', markersize=2, label='Future High Emission')\n",
    "plt.semilogx(aeps, low_emission_sorted_losses, marker='o', linestyle='-', markersize=2, label='Future Low Emission')\n",
    "plt.semilogx(aeps, urban_protection_high_emission_sorted_losses, marker='o', linestyle='-', markersize=2, label='Urban Protection')\n",
    "plt.semilogx(aeps, dry_proofing_high_emission_sorted_losses, marker='o', linestyle='-', markersize=2, label='Dry Proofing')\n",
    "plt.semilogx(aeps, relocation_high_emission_sorted_losses, marker='o', linestyle='-', markersize=2, label='Relocation')\n",
    "plt.xlabel('Annual Exceedance Probability (AEP)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Exceedance Probability Curve')\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5d3fd30-3d37-43c7-8b08-7b66795708c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m dry_proofing_losses_b \u001b[38;5;241m=\u001b[39m dry_proofing_high_emission_sorted_losses\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1000000000\u001b[39m\n\u001b[0;32m      6\u001b[0m relocation_losses_b \u001b[38;5;241m=\u001b[39m relocation_high_emission_sorted_losses\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1000000000\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# plt.plot(aeps, baseline_losses_b, marker='.', linestyle='-', label='Baseline')\u001b[39;00m\n\u001b[0;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(aeps, high_emissions_losses_b, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBaseline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "baseline_losses_b = baseline_sorted_losses/1000000000\n",
    "high_emissions_losses_b = high_emission_sorted_losses/1000000000\n",
    "low_emissions_losses_b = low_emission_sorted_losses/1000000000\n",
    "urban_protection_losses_b = urban_protection_high_emission_sorted_losses/1000000000\n",
    "dry_proofing_losses_b = dry_proofing_high_emission_sorted_losses/1000000000\n",
    "relocation_losses_b = relocation_high_emission_sorted_losses/1000000000\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "# plt.plot(aeps, baseline_losses_b, marker='.', linestyle='-', label='Baseline')\n",
    "plt.plot(aeps, high_emissions_losses_b, marker='.', linestyle='-', label='Baseline')\n",
    "plt.plot(aeps, urban_protection_losses_b, marker='.', linestyle='-', label='Urban Protection')\n",
    "plt.plot(aeps, dry_proofing_losses_b, marker='.', linestyle='-', label='Dry Proofing')\n",
    "plt.plot(aeps, relocation_losses_b, marker='.', linestyle='-', label='Relocation')\n",
    "# plt.plot(aeps, low_emissions_losses_b, marker='.', linestyle='-', label='Low Emissions')\n",
    "# plt.xscale('log')\n",
    "plt.ylabel('Loss ($b)')\n",
    "plt.xlabel('Probability')\n",
    "plt.xlim(0, 0.5)\n",
    "plt.ylim(0, 200)\n",
    "plt.legend()\n",
    "plt.title('Loss Exceedance Probability Curve')\n",
    "plt.grid(True, which='both', ls='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "353c8ce1-2c71-4b27-a3af-daf1ddf07fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_for_rp(aeps, losses, rp):\n",
    "    target_aep = 1 / rp\n",
    "    # Find the closest AEP in the array to the target AEP and get the corresponding loss\n",
    "    idx = (np.abs(aeps - target_aep)).argmin()\n",
    "    return losses[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "470e82eb-f0cf-49fb-851f-13c181450e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186774772143.27112"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_loss_for_rp(aeps, relocation_high_emission_sorted_losses, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "890feab4-3956-4b07-9765-57787ea295c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([259.09285824, 237.52868611, 231.13900166, ...,   0.        ,\n",
       "         0.        ,   0.        ])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relocation_losses_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "38b69865-db98-43e6-98d7-92bb559a5a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.720693074229306"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmean(relocation_losses_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22050003-9e4e-4561-bf10-aa45ce2709b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "copulas",
   "language": "python",
   "name": "copulas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
