{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d66d37f-d5c4-4e0a-93b6-ad3adc1ba125",
   "metadata": {},
   "source": [
    "#### Testing the Copula methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8ed8064-b28f-4787-9b63-1d6164d122d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import xarray as xr\n",
    "# Disable warnings for data download via API\n",
    "import urllib3 \n",
    "urllib3.disable_warnings()\n",
    "# Disable xarray runtime warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lmoments3 import distr\n",
    "from scipy.stats import gumbel_r, kstest\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b8c52-7988-40ae-aa35-9a28ea3d5723",
   "metadata": {},
   "source": [
    "#### Prepare GLOFAS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "090b3396-fe64-42bc-87f9-fffb6616f0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data directory\n",
    "DATADIR = r\"D:\\projects\\sovereign-risk\\Thailand\\data\\flood\\dependence\\glofas\"\n",
    "os.makedirs(DATADIR, exist_ok=True)\n",
    "\n",
    "start_year = 1979\n",
    "end_year = 2023\n",
    "\n",
    "def combine_datasets(start, end, dir):\n",
    "    '''\n",
    "    combine all datasets into one xarray for analysis\n",
    "    '''\n",
    "    all_files = [os.path.join(dir, f\"glofas_THA_{year}.grib\") for year in range(start, end+1)]\n",
    "    # Load all datasets into array\n",
    "    datasets = [xr.open_dataset(file, engine='cfgrib') for file in all_files]\n",
    "    # Concatenate all datasets along the time dimension\n",
    "    combined_dataset = xr.concat(datasets, dim='time')\n",
    "    # Make sure datasets are sorted by time\n",
    "    combined_dataset = combined_dataset.sortby('time')\n",
    "    \n",
    "    return combined_dataset\n",
    "\n",
    "# Load glofas data and combine\n",
    "glofas_data = combine_datasets(start_year, end_year, DATADIR)\n",
    "glofas_data\n",
    "\n",
    "# Reduce the Upstream area data to the domain of the river discharge\n",
    "upstream_area_fname = f\"uparea_glofas_v4_0.nc\"\n",
    "upstream_area_file = os.path.join(DATADIR, upstream_area_fname)\n",
    "# Filter glofas timeseries based on upstream accumulating area\n",
    "area_filter = 500\n",
    "\n",
    "# Open the file and print the contents\n",
    "upstream_area = xr.open_dataset(upstream_area_file, engine='netcdf4')\n",
    "\n",
    "# Get the latitude and longitude limits of the data\n",
    "lat_limits = [glofas_data.latitude.values[i] for i in [0, -1]]\n",
    "lon_limits = [glofas_data.longitude.values[i] for i in [0, -1]]\n",
    "up_lats = upstream_area.latitude.values.tolist()\n",
    "up_lons = upstream_area.longitude.values.tolist()\n",
    "\n",
    "lat_slice_index = [\n",
    "    round((i-up_lats[0])/(up_lats[1]-up_lats[0]))\n",
    "    for i in lat_limits\n",
    "]\n",
    "lon_slice_index = [\n",
    "    round((i-up_lons[0])/(up_lons[1]-up_lons[0]))\n",
    "    for i in lon_limits\n",
    "]\n",
    "\n",
    "# Slice upstream area to Thailand region:\n",
    "red_upstream_area = upstream_area.isel(\n",
    "    latitude=slice(lat_slice_index[0], lat_slice_index[1]+1),\n",
    "    longitude=slice(lon_slice_index[0], lon_slice_index[1]+1),\n",
    ")\n",
    "\n",
    "# There are very minor rounding differences, so we update with the lat/lons from the glofas data\n",
    "red_upstream_area = red_upstream_area.assign_coords({\n",
    "    'latitude': glofas_data.latitude,\n",
    "    'longitude': glofas_data.longitude,\n",
    "})\n",
    "\n",
    "# Add the upstream area to the main data object and print the updated glofas data object:\n",
    "glofas_data['uparea'] = red_upstream_area['uparea']\n",
    "glofas_data\n",
    "\n",
    "# Mask the river discharge data\n",
    "glofas_data_masked = glofas_data.where(glofas_data.uparea>=area_filter*1e6)\n",
    "\n",
    "# Load the basin outlet data\n",
    "basin_outlet_file = r\"D:\\projects\\sovereign-risk\\Thailand\\data\\flood\\dependence\\thailand-basins\\lev06_outlets_final_clipped_Thailand_no_duplicates.csv\"\n",
    "basin_outlet_df = pd.read_csv(basin_outlet_file)\n",
    "# Note to align the two datasets we need to make the following adjustment to lat lons (based on previous trial and error)\n",
    "basin_outlet_df['Latitude'] = basin_outlet_df['Latitude'] + 0.05/2\n",
    "basin_outlet_df['Longitude'] = basin_outlet_df['Longitude'] - 0.05/2\n",
    "\n",
    "# Define function for checking timeseries\n",
    "def check_timeseries(array, latitude, longitude):\n",
    "    test_point = array.sel(latitude=latitude, longitude=longitude, method='nearest')\n",
    "    test_timeseries = test_point['dis24']\n",
    "    test_acc = float(test_point['uparea'])\n",
    "    # check for NaN values\n",
    "    non_nan_count = test_timeseries.count().item()\n",
    "    total_count = test_timeseries.size\n",
    "    nan_ratio = non_nan_count/total_count\n",
    "\n",
    "    # Does the timeseries pass the NaN threshold\n",
    "    if nan_ratio < 1:\n",
    "        return False, test_acc, \"NaN values found\"\n",
    "\n",
    "    # Check for constant values\n",
    "    if test_timeseries.min() == test_timeseries.max():\n",
    "        return False, test_acc, \"Constant timeseries values\"\n",
    "\n",
    "    # If all checks pass\n",
    "    return True, test_acc, \"Valid timeseries\"\n",
    "\n",
    "# Loop through basins and check whether timeseries is valid\n",
    "results = []\n",
    "for index, row in basin_outlet_df.iterrows():\n",
    "    latitude = row['Latitude']\n",
    "    longitude = row['Longitude']\n",
    "\n",
    "    valid, acc, message = check_timeseries(glofas_data_masked, latitude, longitude)\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'HYBAS_ID': row['HYBAS_ID'],\n",
    "        'Latitude': latitude,\n",
    "        'Longitude': longitude,\n",
    "        'Acc': acc,\n",
    "        'Valid': valid,\n",
    "        'Message': message\n",
    "    })\n",
    "    if not valid:\n",
    "        print(f\"ID: {row['HYBAS_ID']}, Lat: {latitude}, Lon: {longitude}, Acc: {acc}, Valid: {valid}, Message: {message}\")\n",
    "\n",
    "# over what years do we want to extract the data?\n",
    "start_year = 1979\n",
    "end_year = 2016\n",
    "sliced_data = glofas_data_masked.sel(time=slice(str(start_year), str(end_year)))\n",
    "# Dictionary to store timeseries data for each basin\n",
    "basin_timeseries = {}\n",
    "\n",
    "# Loop through basin outlets, storing each in turn\n",
    "for index, row in basin_outlet_df.iterrows():\n",
    "    basin_id = row['HYBAS_ID']\n",
    "    lat = row['Latitude']\n",
    "    lon = row['Longitude']\n",
    "    point_data = sliced_data.sel(latitude=lat, longitude=lon, method='nearest')\n",
    "    timeseries = point_data['dis24'].to_series()\n",
    "    # store in dictionary\n",
    "    basin_timeseries[basin_id] = timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404fbb9a-9d2c-498a-857f-85394c5724a6",
   "metadata": {},
   "source": [
    "#### Perform EVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1df2ec3a-6305-46a0-9184-c950b86658a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store fitted parameters for each basin\n",
    "gumbel_params = {}\n",
    "fit_quality = {}\n",
    "\n",
    "# Loop through basins, calculating annual maxima and fitting Gumbel distribution using L-moments\n",
    "for basin_id, timeseries in basin_timeseries.items():\n",
    "    annual_maxima = timeseries.groupby(timeseries.index.year).max()\n",
    "\n",
    "    # Fit Gumbel distribution using L-moments\n",
    "    params = distr.gum.lmom_fit(annual_maxima)\n",
    "\n",
    "    # Perform the Kolmogorov-Smirnov test (checking quality of fit)\n",
    "    D, p_value = kstest(annual_maxima, 'gumbel_r', args=(params['loc'], params['scale']))\n",
    "\n",
    "    gumbel_params[basin_id] = params\n",
    "    fit_quality[basin_id] = (D, p_value)\n",
    "\n",
    "# Will do this using the CDF of the fitted Gumbel distribution \n",
    "\n",
    "# Dictionary to story uniform marginals for each basin\n",
    "uniform_marginals = {}\n",
    "\n",
    "for basin_id, timeseries in basin_timeseries.items():\n",
    "    annual_maxima = timeseries.groupby(timeseries.index.year).max()\n",
    "    params = gumbel_params[basin_id]\n",
    "    uniform_marginals[basin_id] = gumbel_r.cdf(annual_maxima, loc=params['loc'], scale=params['scale'])\n",
    "\n",
    "basin_ids = list(uniform_marginals.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105784e6-7a9c-4c02-bd35-baadd60fe643",
   "metadata": {},
   "source": [
    "#### Run Copula Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e98ef582-dd7e-49a9-b652-91a959b239c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copulas.bivariate import Clayton\n",
    "clayton_copula_models = {}\n",
    "clayton_error_basins = [] # list to store basins that cause an error\n",
    "\n",
    "for id1, margins1 in uniform_marginals.items():\n",
    "    for id2, margins2 in uniform_marginals.items():\n",
    "        if id1 < id2: # to avoid duplicate pairs\n",
    "            try:\n",
    "                # Prepare the data for copula\n",
    "                data = np.column_stack((1-margins1, 1-margins2)) # interested in upper tail dependence so take inverse of CDF\n",
    "                \n",
    "                # Fit the Clayton copula\n",
    "                flipped_clayton = Clayton()\n",
    "                flipped_clayton.fit(data)\n",
    "    \n",
    "                # Store the copula model\n",
    "                clayton_copula_models[(id1, id2)] = flipped_clayton\n",
    "            except ValueError as e:\n",
    "                # print(f\"Error fitting Clayton copula for basins {id1} and {id2}: {e}\")\n",
    "                clayton_error_basins.append((id1, id2))\n",
    "\n",
    "# Store these copula pairs in a matrix\n",
    "basin_ids = list(uniform_marginals.keys()) # take the basin IDs from the uniform marginals dictionary\n",
    "N = len(basin_ids)\n",
    "\n",
    "# Initialize the matrix with NaNs\n",
    "dependence_matrix = np.full((N, N), np.nan)\n",
    "\n",
    "# Map from basin ID to matrix index\n",
    "id_to_index = {basin_id: index for index, basin_id in enumerate(basin_ids)}\n",
    "\n",
    "for (id1, id2), copula_model in clayton_copula_models.items():\n",
    "    index1, index2 = id_to_index[id1], id_to_index[id2]\n",
    "    dependence_matrix[index1, index2] = copula_model.theta\n",
    "    dependence_matrix[index2, index1] = copula_model.theta\n",
    "\n",
    "# For error basins do the same but set theta to -1\n",
    "for (id1, id2) in clayton_error_basins:\n",
    "    index1, index2 = id_to_index[id1], id_to_index[id2]\n",
    "    dependence_matrix[index1, index2] = -1\n",
    "    dependence_matrix[index2, index1] = -1\n",
    "\n",
    "# Debug (for infinity values) - not sure if needed but there are a few where I had to reassign basin outlets.\n",
    "dependence_matrix[np.isinf(dependence_matrix)] = 1000\n",
    "                \n",
    "# Step 1: Find the most dependent pair\n",
    "# Initialize a set to keep track of selected basin indices\n",
    "selected_indices = set()\n",
    "# convert dependence_matrix to a masked array, so that NaN values and -1 are not considered in the operation\n",
    "masked_dependence_matrix = np.ma.masked_less(dependence_matrix, 0) # masking out values < 0\n",
    "np.fill_diagonal(masked_dependence_matrix, np.ma.masked) # we want to ignore diagonal (NaN values)\n",
    "max_theta_index = np.unravel_index(np.argmax(masked_dependence_matrix, axis=None), masked_dependence_matrix.shape)\n",
    "ordered_basins = [basin_ids[max_theta_index[0]], basin_ids[max_theta_index[1]]]\n",
    "# Add indices to the set of selected indices\n",
    "selected_indices.update([max_theta_index[0], max_theta_index[1]])\n",
    "\n",
    "# Step 2-4: Loop until all basins are ordered\n",
    "while len(ordered_basins) < len(basin_ids):\n",
    "    # Step 2: Choose basin k that is dependent on both basin i, j (last two basins in ordered_basins). Minimax approach\n",
    "    # Exclude already selected basins from the selection process\n",
    "    potential_next_indices = [i for i in range(len(basin_ids)) if i not in selected_indices]\n",
    "    # Find the indices of the last two basins in ordered_basins\n",
    "    last_two_indices = [id_to_index[basin] for basin in ordered_basins[-2:]]\n",
    "    # Find dependency vectors for the last two basins\n",
    "    dependency_vectors = masked_dependence_matrix[last_two_indices, :]\n",
    "    # Calculate the minimum dependency for each row of the vector\n",
    "    min_deps = np.ma.min(dependency_vectors, axis=0)\n",
    "    # Mask already selected indices\n",
    "    min_deps_masked = np.ma.copy(min_deps)\n",
    "    # Debug \n",
    "    # Ensure min_deps_masked.mask is an array\n",
    "    if np.isscalar(min_deps_masked.mask):\n",
    "        min_deps_masked.mask = np.zeros(min_deps_masked.shape, dtype=bool)\n",
    "    for idx in selected_indices:\n",
    "        min_deps_masked.mask[idx] = True # mask the index if it's already in selected indices\n",
    "    # Step 3: Find the maximum dependency value over the minimized vector - which will be the next basin\n",
    "    next_basin_index = np.ma.argmax(min_deps_masked, fill_value=-np.inf)\n",
    "    # Step 4: Continue iterations until there are no more basins left to process\n",
    "    # Check if all options are effectively masked\n",
    "    if min_deps_masked.mask.all():\n",
    "        print(\"No suitable next basin found. Ending process.\")\n",
    "        break\n",
    "    next_basin = basin_ids[next_basin_index]\n",
    "    ordered_basins.append(next_basin)\n",
    "    selected_indices.add(next_basin_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f7b8d-cca5-4c7a-bf53-b73a9ab9dfd3",
   "metadata": {},
   "source": [
    "#### Run risk analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a1be564-42f5-4474-998a-68d3a18d85a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_damages(RPs, losses, sim_aep, protection_level=0.5):\n",
    "    aeps = [1/i for i in RPs]\n",
    "    # Ensure AEPs are in ascending order for np.interp\n",
    "    aeps.sort() \n",
    "    print(losses)\n",
    "    losses = losses[::-1]\n",
    "\n",
    "    # test\n",
    "    # sim_aep = 1 - sim_aep # convert extreme simulated AEPs (e.g. 0.95) to equivalent AEPs for interpolation (e.g. 0.05)\n",
    "\n",
    "    # Interpolate based off simulated AEP\n",
    "    if sim_aep >= protection_level: \n",
    "        return 0 \n",
    "    else:\n",
    "        interpolated_value = np.interp(sim_aep, aeps, losses)\n",
    "        return interpolated_value\n",
    "\n",
    "def basin_loss_curve(loss_df, basin_id, basin_col, rps):\n",
    "    losses = {} # initialize empty dictionary to store losses and protection level\n",
    "    basin_df = loss_df[loss_df[basin_col]==basin_id]\n",
    "    grouped_basin_df = basin_df.groupby([basin_col, 'RP', 'Pr_L_AEP']).agg({'damages':'sum'}).reset_index()\n",
    "    # # Pull unique protection levels from the grouped dataframe\n",
    "    unique_protection_levels = grouped_basin_df['Pr_L_AEP'].unique()\n",
    "    if len(unique_protection_levels) == 0:\n",
    "        unique_protection_levels = [1]\n",
    "    for i in unique_protection_levels:\n",
    "        losses[i] = [grouped_basin_df.loc[(grouped_basin_df['RP'] == rp) & (grouped_basin_df['Pr_L_AEP']==i), 'damages'].sum() for rp in rps]\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77915fb2-16ff-43b9-856d-1c3fdf1aff1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>FID</th>\n",
       "      <th>GID_1</th>\n",
       "      <th>NAME</th>\n",
       "      <th>HB_L4</th>\n",
       "      <th>HB_L5</th>\n",
       "      <th>HB_L6</th>\n",
       "      <th>HB_L7</th>\n",
       "      <th>Pr_L</th>\n",
       "      <th>Add_Pr</th>\n",
       "      <th>New_Pr_L</th>\n",
       "      <th>damages</th>\n",
       "      <th>RP</th>\n",
       "      <th>AEP</th>\n",
       "      <th>Pr_L_AEP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>THA.62_1</td>\n",
       "      <td>Si Sa Ket</td>\n",
       "      <td>4.041145e+09</td>\n",
       "      <td>4.051145e+09</td>\n",
       "      <td>4.061140e+09</td>\n",
       "      <td>4.071125e+09</td>\n",
       "      <td>16.387501</td>\n",
       "      <td>83.612499</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.061022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>THA.62_1</td>\n",
       "      <td>Si Sa Ket</td>\n",
       "      <td>4.041145e+09</td>\n",
       "      <td>4.051145e+09</td>\n",
       "      <td>4.061140e+09</td>\n",
       "      <td>4.071125e+09</td>\n",
       "      <td>16.387501</td>\n",
       "      <td>83.612499</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.061022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>THA.62_1</td>\n",
       "      <td>Si Sa Ket</td>\n",
       "      <td>4.041145e+09</td>\n",
       "      <td>4.051145e+09</td>\n",
       "      <td>4.061131e+09</td>\n",
       "      <td>4.071121e+09</td>\n",
       "      <td>16.387501</td>\n",
       "      <td>83.612499</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.061022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>THA.62_1</td>\n",
       "      <td>Si Sa Ket</td>\n",
       "      <td>4.041109e+09</td>\n",
       "      <td>4.051109e+09</td>\n",
       "      <td>4.061109e+09</td>\n",
       "      <td>4.071109e+09</td>\n",
       "      <td>16.387501</td>\n",
       "      <td>83.612499</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.106698e+05</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.061022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>THA.62_1</td>\n",
       "      <td>Si Sa Ket</td>\n",
       "      <td>4.041109e+09</td>\n",
       "      <td>4.051110e+09</td>\n",
       "      <td>4.061110e+09</td>\n",
       "      <td>4.071111e+09</td>\n",
       "      <td>16.387501</td>\n",
       "      <td>83.612499</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.922638e+06</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.061022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  FID     GID_1       NAME         HB_L4         HB_L5  \\\n",
       "0           0    0  THA.62_1  Si Sa Ket  4.041145e+09  4.051145e+09   \n",
       "1           1    1  THA.62_1  Si Sa Ket  4.041145e+09  4.051145e+09   \n",
       "2           2    2  THA.62_1  Si Sa Ket  4.041145e+09  4.051145e+09   \n",
       "3           3    3  THA.62_1  Si Sa Ket  4.041109e+09  4.051109e+09   \n",
       "4           4    4  THA.62_1  Si Sa Ket  4.041109e+09  4.051110e+09   \n",
       "\n",
       "          HB_L6         HB_L7       Pr_L     Add_Pr  New_Pr_L       damages  \\\n",
       "0  4.061140e+09  4.071125e+09  16.387501  83.612499     100.0  0.000000e+00   \n",
       "1  4.061140e+09  4.071125e+09  16.387501  83.612499     100.0  0.000000e+00   \n",
       "2  4.061131e+09  4.071121e+09  16.387501  83.612499     100.0  0.000000e+00   \n",
       "3  4.061109e+09  4.071109e+09  16.387501  83.612499     100.0  1.106698e+05   \n",
       "4  4.061110e+09  4.071111e+09  16.387501  83.612499     100.0  2.922638e+06   \n",
       "\n",
       "   RP  AEP  Pr_L_AEP  \n",
       "0   2  0.5  0.061022  \n",
       "1   2  0.5  0.061022  \n",
       "2   2  0.5  0.061022  \n",
       "3   2  0.5  0.061022  \n",
       "4   2  0.5  0.061022  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load risk data\n",
    "risk_data = pd.read_csv(r\"D:\\projects\\sovereign-risk\\Thailand\\test\\copulas\\risk_basin_zonal_sum.csv\")\n",
    "# Add columne for annual exceedance probability\n",
    "risk_data['AEP'] = 1 / risk_data['RP']\n",
    "# Add a column converting current prorection level into AEP\n",
    "risk_data['Pr_L_AEP'] = np.where(risk_data['Pr_L'] == 0, 0, 1 / risk_data['Pr_L']) # using numpy where avoids zero division errors\n",
    "rps = [2, 5, 10, 25, 50, 100, 200, 500, 1000]\n",
    "risk_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1a22484-9329-455c-b92b-985b895ecc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 70\u001b[0m\n\u001b[0;32m     66\u001b[0m             national_losses_per_year[simulation, year] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(yearly_loss_values)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(national_losses_per_year, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_years)])\n\u001b[1;32m---> 70\u001b[0m \u001b[43mmonte_carlo_dependence_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrisk_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHB_L6\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 46\u001b[0m, in \u001b[0;36mmonte_carlo_dependence_simulation\u001b[1;34m(loss_df, rps, basin_col, protection_level, num_years, num_simulations)\u001b[0m\n\u001b[0;32m     44\u001b[0m     loss_curve \u001b[38;5;241m=\u001b[39m basin_loss_curves[basin_id]\n\u001b[0;32m     45\u001b[0m     yearly_losses\u001b[38;5;241m.\u001b[39mappend(r) \u001b[38;5;66;03m# will store losses as AEP\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     yearly_loss_values\u001b[38;5;241m.\u001b[39mappend(\u001b[43minterpolate_damages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_curve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# store losses as interpolated values\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     loss_curve \u001b[38;5;241m=\u001b[39m basin_loss_curves[basin_id]\n",
      "Cell \u001b[1;32mIn[28], line 6\u001b[0m, in \u001b[0;36minterpolate_damages\u001b[1;34m(RPs, losses, sim_aep, protection_level)\u001b[0m\n\u001b[0;32m      4\u001b[0m aeps\u001b[38;5;241m.\u001b[39msort() \n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(losses)\n\u001b[1;32m----> 6\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mlosses\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# test\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# sim_aep = 1 - sim_aep # convert extreme simulated AEPs (e.g. 0.95) to equivalent AEPs for interpolation (e.g. 0.05)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Interpolate based off simulated AEP\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sim_aep \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m protection_level: \n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "def monte_carlo_dependence_simulation(\n",
    "        loss_df, \n",
    "        rps, \n",
    "        basin_col, \n",
    "        protection_level, \n",
    "        num_years, \n",
    "        num_simulations=1000):\n",
    "    '''\n",
    "    Adjusted to account for urban protection\n",
    "    Perform Monte Carlo simulations of yearly losses incorporating basin dependencies. This function is specifically for simulating urban flood protection\n",
    "\n",
    "    :param loss_df: dataframe with losses from risk analysis\n",
    "    :param rps: list of return periods to consider. \n",
    "    :param basin_col: name of column for basins (e.g. 'HB_L6')\n",
    "    :param protection_level: what is the baseline protection level (e.g. 0.5 or 1 in 2 years)\n",
    "    :param num_years: Number of years to simulate\n",
    "    :param ordered_basins: List of basin IDs ordered by dependency\n",
    "    :param copula_models: Dictionary holding copula model for each basin pair\n",
    "    :param gumbel_params: Gumbel distribution parameters for each basin.\n",
    "    :param num__simulations: Number of simulations (default is 10,000).\n",
    "    :return: Dataframe of simulated national losses for each year.\n",
    "    '''\n",
    "    \n",
    "    # To speed up the Monte-Carlo simulation we are going to pre-compute some variables\n",
    "    # precompute loss-probability curves for each basin\n",
    "    basin_loss_curves = {basin_id: basin_loss_curve(loss_df, basin_id, basin_col, rps) for basin_id in ordered_basins}\n",
    "    # Initialize array for national losses\n",
    "    national_losses_per_year = np.zeros((num_simulations, num_years))\n",
    "    # Generate all random numbers in advance\n",
    "    random_numbers = np.random.uniform(0, 1, (num_simulations, num_years, len(ordered_basins))).astype(np.float32)\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for sim in tqdm(range(num_simulations)):\n",
    "        for year in range(num_years):\n",
    "            # Initialize a list to store losses for each basin for the current year\n",
    "            yearly_losses = []\n",
    "            yearly_loss_values = []\n",
    "            i += 1\n",
    "            for i, basin_id in enumerate(ordered_basins):\n",
    "                if i == 0:\n",
    "                    # Handle first basin\n",
    "                    r = random_numbers[sim, year, i]\n",
    "                    loss_curve = basin_loss_curves[basin_id]\n",
    "                    yearly_losses.append(r) # will store losses as AEP\n",
    "                    yearly_loss_values.append(interpolate_damages(rps, loss_curve, r)) # store losses as interpolated values\n",
    "                else:\n",
    "                    loss_curve = basin_loss_curves[basin_id]\n",
    "                    # Handle subsequent basins with dependencies\n",
    "                    copula = get_copula_model(copula_models, ordered_basins[i-1], basin_id)\n",
    "                    if copula is not None:\n",
    "                        # Apply dependency model if theta exists\n",
    "                        r = random_numbers[sim, year, i]\n",
    "                        previous_loss = yearly_losses[i-1]\n",
    "                        current_loss = generate_conditional_sample(previous_loss, copula.theta, r)\n",
    "                        # print(copula.theta, previous_loss, r, current_loss)\n",
    "                        yearly_losses.append(current_loss)\n",
    "                        yearly_loss_values.append(interpolate_damages(rps, loss_curve, (1-current_loss))) # here we are inverting the random number to AEP to simulate tail dependence\n",
    "                    else:\n",
    "                        # Independent simulation for this basin\n",
    "                        r = random_numbers[sim, year, i]\n",
    "                        yearly_losses.append(r)\n",
    "                        yearly_loss_values.append(interpolate_damages(rps, loss_curve, r))\n",
    "\n",
    "            # Aggregate losses for the current year\n",
    "            national_losses_per_year[simulation, year] = sum(yearly_loss_values)\n",
    "            \n",
    "    return pd.DataFrame(national_losses_per_year, columns=[f'Year_{i+1}' for i in range(num_years)])\n",
    "\n",
    "monte_carlo_dependence_simulation(risk_data, rps, 'HB_L6',0.5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0295fa1-f194-4e59-8918-76216f5bc959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sovereign-risk",
   "language": "python",
   "name": "sovereign-risk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
