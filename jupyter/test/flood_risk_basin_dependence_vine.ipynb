{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9c1baf-945a-444d-87b3-c24be624c80a",
   "metadata": {},
   "source": [
    "### Step 1: Do all the dependence analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd019295-54ef-41dc-9add-8dad9cd74dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import xarray as xr\n",
    "# Disable warnings for data download via API\n",
    "import urllib3 \n",
    "urllib3.disable_warnings()\n",
    "# Disable xarray runtime warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lmoments3 import distr\n",
    "from scipy.stats import gumbel_r, kstest\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6831a09-7414-4ff1-aa5f-8ad311c031b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate pairwise basin dependence\n",
    "# Set up CDS API key\n",
    "# Set up data directory\n",
    "DATADIR = r\"/Users/rubenkerkhofs/Desktop/glofas\"\n",
    "os.makedirs(DATADIR, exist_ok=True)\n",
    "\n",
    "# Download data for Thaiand (the GIRI model uses the historical time period 1979-2016). We will pull data from 1979-2023\n",
    "start_year = 1979\n",
    "end_year = 2023\n",
    "# new_dir = os.path.join(DATADIR, f\"THA_{start_year}-{end_year}\")\n",
    "# os.makedirs(new_dir)\n",
    "for year in range(start_year, end_year+1):\n",
    "    download_file = f\"{DATADIR}/glofas_THA_{year}.grib\"\n",
    "\n",
    "# Download the upstream area\n",
    "# NOTE: issue downloading a valid netcdf in current script. Workaround at the moment is using file I've previously downloaded\n",
    "upstream_area_fname = f\"uparea_glofas_v4_0.nc\"\n",
    "upstream_area_file = os.path.join(DATADIR, upstream_area_fname)\n",
    "# If we have not already downloaded the data, download it.\n",
    "if not os.path.isfile(upstream_area_file):\n",
    "    u_version=2 # file version\n",
    "    upstream_data_url = (\n",
    "        f\"https://confluence.ecmwf.int/download/attachments/242067380/{upstream_area_file}?\"\n",
    "        f\"version{u_version}&modificationDate=1668604690076&api=v2&download=true\"\n",
    "    )\n",
    "    import requests\n",
    "    result = requests.get(upstream_data_url)\n",
    "    with open(upstream_area_file, 'wb') as f:\n",
    "        f.write(result.content)\n",
    "\n",
    "def combine_datasets(start, end, dir):\n",
    "    '''\n",
    "    combine all datasets into one xarray for analysis\n",
    "    '''\n",
    "    all_files = [os.path.join(dir, f\"glofas_THA_{year}.grib\") for year in range(start, end+1)]\n",
    "    # Load all datasets into array\n",
    "    datasets = [xr.open_dataset(file, engine='cfgrib') for file in all_files]\n",
    "    # Concatenate all datasets along the time dimension\n",
    "    combined_dataset = xr.concat(datasets, dim='time')\n",
    "    # Make sure datasets are sorted by time\n",
    "    combined_dataset = combined_dataset.sortby('time')\n",
    "    \n",
    "    return combined_dataset\n",
    "\n",
    "# Load glofas data and combine\n",
    "glofas_data = combine_datasets(start_year, end_year, DATADIR)\n",
    "\n",
    "# Reduce the Upstream area data to the domain of the river discharge\n",
    "\n",
    "# Filter glofas timeseries based on upstream accumulating area\n",
    "area_filter = 500\n",
    "\n",
    "# Open the file and print the contents\n",
    "upstream_area = xr.open_dataset(upstream_area_file, engine='netcdf4')\n",
    "\n",
    "# Get the latitude and longitude limits of the data\n",
    "lat_limits = [glofas_data.latitude.values[i] for i in [0, -1]]\n",
    "lon_limits = [glofas_data.longitude.values[i] for i in [0, -1]]\n",
    "up_lats = upstream_area.latitude.values.tolist()\n",
    "up_lons = upstream_area.longitude.values.tolist()\n",
    "\n",
    "lat_slice_index = [\n",
    "    round((i-up_lats[0])/(up_lats[1]-up_lats[0]))\n",
    "    for i in lat_limits\n",
    "]\n",
    "lon_slice_index = [\n",
    "    round((i-up_lons[0])/(up_lons[1]-up_lons[0]))\n",
    "    for i in lon_limits\n",
    "]\n",
    "\n",
    "# Slice upstream area to Thailand region:\n",
    "red_upstream_area = upstream_area.isel(\n",
    "    latitude=slice(lat_slice_index[0], lat_slice_index[1]+1),\n",
    "    longitude=slice(lon_slice_index[0], lon_slice_index[1]+1),\n",
    ")\n",
    "\n",
    "# There are very minor rounding differences, so we update with the lat/lons from the glofas data\n",
    "red_upstream_area = red_upstream_area.assign_coords({\n",
    "    'latitude': glofas_data.latitude,\n",
    "    'longitude': glofas_data.longitude,\n",
    "})\n",
    "\n",
    "# Add the upstream area to the main data object and print the updated glofas data object:\n",
    "glofas_data['uparea'] = red_upstream_area['uparea']\n",
    "glofas_data\n",
    "\n",
    "# Mask the river discharge data\n",
    "glofas_data_masked = glofas_data.where(glofas_data.uparea>=area_filter*1e6)\n",
    "\n",
    "# Load the basin outlet data\n",
    "basin_outlet_file = r\"/Users/rubenkerkhofs/Desktop/glofas/lev06_outlets_final_clipped_Thailand_no_duplicates.csv\"\n",
    "basin_outlet_df = pd.read_csv(basin_outlet_file)\n",
    "# Note to align the two datasets we need to make the following adjustment to lat lons (based on previous trial and error)\n",
    "basin_outlet_df['Latitude'] = basin_outlet_df['Latitude'] + 0.05/2\n",
    "basin_outlet_df['Longitude'] = basin_outlet_df['Longitude'] - 0.05/2\n",
    "\n",
    "# Define function for checking timeseries\n",
    "def check_timeseries(array, latitude, longitude):\n",
    "    test_point = array.sel(latitude=latitude, longitude=longitude, method='nearest')\n",
    "    test_timeseries = test_point['dis24']\n",
    "    test_acc = float(test_point['uparea'])\n",
    "    # check for NaN values\n",
    "    non_nan_count = test_timeseries.count().item()\n",
    "    total_count = test_timeseries.size\n",
    "    nan_ratio = non_nan_count/total_count\n",
    "\n",
    "    # Does the timeseries pass the NaN threshold\n",
    "    if nan_ratio < 1:\n",
    "        return False, test_acc, \"NaN values found\"\n",
    "\n",
    "    # Check for constant values\n",
    "    if test_timeseries.min() == test_timeseries.max():\n",
    "        return False, test_acc, \"Constant timeseries values\"\n",
    "\n",
    "    # If all checks pass\n",
    "    return True, test_acc, \"Valid timeseries\"\n",
    "\n",
    "# Loop through basins and check whether timeseries is valid\n",
    "results = []\n",
    "for index, row in basin_outlet_df.iterrows():\n",
    "    latitude = row['Latitude']\n",
    "    longitude = row['Longitude']\n",
    "\n",
    "    valid, acc, message = check_timeseries(glofas_data_masked, latitude, longitude)\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'HYBAS_ID': row['HYBAS_ID'],\n",
    "        'Latitude': latitude,\n",
    "        'Longitude': longitude,\n",
    "        'Acc': acc,\n",
    "        'Valid': valid,\n",
    "        'Message': message\n",
    "    })\n",
    "    if not valid:\n",
    "        print(f\"ID: {row['HYBAS_ID']}, Lat: {latitude}, Lon: {longitude}, Acc: {acc}, Valid: {valid}, Message: {message}\")\n",
    "\n",
    "# over what years do we want to extract the data?\n",
    "start_year = 1979\n",
    "end_year = 2016\n",
    "sliced_data = glofas_data_masked.sel(time=slice(str(start_year), str(end_year)))\n",
    "# Dictionary to store timeseries data for each basin\n",
    "basin_timeseries = {}\n",
    "\n",
    "# Loop through basin outlets, storing each in turn\n",
    "for index, row in basin_outlet_df.iterrows():\n",
    "    basin_id = row['HYBAS_ID']\n",
    "    lat = row['Latitude']\n",
    "    lon = row['Longitude']\n",
    "    point_data = sliced_data.sel(latitude=lat, longitude=lon, method='nearest')\n",
    "    timeseries = point_data['dis24'].to_series()\n",
    "    # store in dictionary\n",
    "    basin_timeseries[basin_id] = timeseries\n",
    "\n",
    "# Dictionary to store fitted parameters for each basin\n",
    "gumbel_params = {}\n",
    "fit_quality = {}\n",
    "\n",
    "# Loop through basins, calculating annual maxima and fitting Gumbel distribution using L-moments\n",
    "for basin_id, timeseries in basin_timeseries.items():\n",
    "    annual_maxima = timeseries.groupby(timeseries.index.year).max()\n",
    "\n",
    "    # Fit Gumbel distribution using L-moments\n",
    "    params = distr.gum.lmom_fit(annual_maxima)\n",
    "\n",
    "    # Perform the Kolmogorov-Smirnov test (checking quality of fit)\n",
    "    D, p_value = kstest(annual_maxima, 'gumbel_r', args=(params['loc'], params['scale']))\n",
    "\n",
    "    gumbel_params[basin_id] = params\n",
    "    fit_quality[basin_id] = (D, p_value)\n",
    "\n",
    "# Will do this using the CDF of the fitted Gumbel distribution \n",
    "\n",
    "# Dictionary to story uniform marginals for each basin\n",
    "uniform_marginals = {}\n",
    "\n",
    "for basin_id, timeseries in basin_timeseries.items():\n",
    "    annual_maxima = timeseries.groupby(timeseries.index.year).max()\n",
    "    params = gumbel_params[basin_id]\n",
    "    uniform_marginals[basin_id] = gumbel_r.cdf(annual_maxima, loc=params['loc'], scale=params['scale'])\n",
    "\n",
    "basin_ids = list(uniform_marginals.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "542b9974-056a-4616-83aa-a5caba6a750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_damages(RPs, losses, sim_aep, protection_level=0.5):\n",
    "    aeps = [1/i for i in RPs]\n",
    "    # Ensure AEPs are in ascending order for np.interp\n",
    "    aeps.sort() \n",
    "    losses = losses[::-1]\n",
    "\n",
    "    # test\n",
    "    #sim_aep = 1 - sim_aep # convert extreme simulated AEPs (e.g. 0.95) to equivalent AEPs for interpolation (e.g. 0.05)\n",
    "\n",
    "    # Interpolate based off simulated AEP\n",
    "    if sim_aep >= protection_level: \n",
    "        return 0 \n",
    "    else:\n",
    "        interpolated_value = np.interp(sim_aep, aeps, losses)\n",
    "        return interpolated_value\n",
    "\n",
    "\n",
    "def basin_loss_curve(loss_df, basin_id, basin_col, epoch_val, scenario_val, urban_class, rps):\n",
    "    losses = {} # initialize empty dictionary to store losses and protection level\n",
    "    basin_df = loss_df[(loss_df[basin_col]==basin_id) & (loss_df['epoch']==epoch_val) & (loss_df['adaptation_scenario']==scenario_val) & (loss_df['urban_class']==urban_class)]\n",
    "    grouped_basin_df = basin_df.groupby([basin_col, 'RP', 'Pr_L_AEP']).agg({'damages':'sum'}).reset_index()\n",
    "    # # Pull unique protection levels from the grouped dataframe\n",
    "    unique_protection_levels = grouped_basin_df['Pr_L_AEP'].unique()\n",
    "    if len(unique_protection_levels) == 0:\n",
    "        unique_protection_levels = [1]\n",
    "    for i in unique_protection_levels:\n",
    "        losses[i] = [grouped_basin_df.loc[(grouped_basin_df['RP'] == rp) & (grouped_basin_df['Pr_L_AEP']==i), 'damages'].sum() for rp in rps]\n",
    "    return losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a656682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load risk data\n",
    "risk_data = pd.read_csv(r\"/Users/rubenkerkhofs/Desktop/glofas/risk_basin_zonal_sum.csv\")\n",
    "# Add columne for annual exceedance probability\n",
    "risk_data['AEP'] = 1 / risk_data['RP']\n",
    "# Add a column converting current prorection level into AEP\n",
    "risk_data['Pr_L_AEP'] = np.where(risk_data['Pr_L'] == 0, 0, 1 / risk_data['Pr_L']) # using numpy where avoids zero division errors\n",
    "#### Add row for each combination that sums residential and non-residential damages\n",
    "grouped = risk_data.groupby(['FID', 'GID_1', 'NAME', 'HB_L4', 'HB_L5', 'HB_L6', 'HB_L7', 'Pr_L', 'Pr_L_AEP', 'Add_Pr', 'New_Pr_L', 'epoch', 'adaptation_scenario', 'RP', 'AEP'], as_index=False)['damages'].sum()\n",
    "grouped['urban_class'] = 'Combined'  # Add a column for urban_class with value 'total'\n",
    "risk_data = pd.concat([risk_data, grouped], ignore_index=True).sort_values(by=['FID', 'GID_1', 'NAME', 'HB_L4', 'HB_L5', 'HB_L6', 'HB_L7', 'Pr_L', 'Pr_L_AEP', 'Add_Pr', 'New_Pr_L', 'epoch', 'adaptation_scenario', 'RP', 'AEP'])\n",
    "risk_data.reset_index(drop=True, inplace=True)\n",
    "rps = [2, 5, 10, 25, 50, 100, 200, 500, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92242594",
   "metadata": {},
   "outputs": [],
   "source": [
    "vine_random_numbers = pd.read_parquet('../Data/vine_random_numbers.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a22bce58-bd7e-4e7d-b3f8-7093d41b55f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_dependence_simulation(\n",
    "        loss_df, \n",
    "        rps, \n",
    "        basin_col, \n",
    "        epoch_val, \n",
    "        scenario_val, \n",
    "        urban_class, \n",
    "        protection_level, \n",
    "        num_years, \n",
    "        urban=False, \n",
    "        num_simulations=1000):\n",
    "    '''\n",
    "    Adjusted to account for urban protection\n",
    "    Perform Monte Carlo simulations of yearly losses incorporating basin dependencies. This function is specifically for simulating urban flood protection\n",
    "\n",
    "    :param loss_df: dataframe with losses from risk analysis\n",
    "    :param rps: list of return periods to consider. \n",
    "    :param basin_col: name of column for basins (e.g. 'HB_L6')\n",
    "    :param epoch_val: name of epoch value (e.g. 'Today')\n",
    "    :param scenario_val: name of scenario (e.g. 'Baseline')\n",
    "    :param urban_class: name of urban class to consider (e.g. 'Residential')\n",
    "    :param protection_level: what is the baseline protection level (e.g. 0.5 or 1 in 2 years)\n",
    "    :param num_years: Number of years to simulate\n",
    "    :param ordered_basins: List of basin IDs ordered by dependency\n",
    "    :param copula_models: Dictionary holding copula model for each basin pair\n",
    "    :param gumbel_params: Gumbel distribution parameters for each basin.\n",
    "    :param num__simulations: Number of simulations (default is 10,000).\n",
    "    :return: Dataframe of simulated national losses for each year.\n",
    "    '''\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # To speed up the Monte-Carlo simulation we are going to pre-compute some variables\n",
    "    # precompute loss-probability curves for each basin\n",
    "    urban_basin_loss_curves = {basin_id: basin_loss_curve(loss_df, basin_id, basin_col, epoch_val, scenario_val, urban_class, rps) for basin_id in basin_ids}\n",
    "    basin_loss_curves = {basin_id: basin_loss_curve(loss_df, basin_id, basin_col, epoch_val,scenario_val , urban_class, rps) for basin_id in basin_ids}\n",
    "    national_losses_per_year = np.zeros((num_simulations, num_years))\n",
    "    random_numbers = vine_random_numbers.sample(n=num_simulations*num_years, replace=True).reset_index(drop=True)\n",
    "    i = 0\n",
    "\n",
    "    for sim in tqdm(range(num_simulations)):\n",
    "        for year in range(num_years):\n",
    "            random_ns = random_numbers.loc[i]\n",
    "            i += 1\n",
    "            yearly_loss_values = []\n",
    "            for _, basin_id in enumerate(basin_ids):\n",
    "                urban_loss_curves = urban_basin_loss_curves[basin_id]\n",
    "                loss_curves = basin_loss_curves[basin_id]\n",
    "                r = 1-random_ns[str(basin_id)]\n",
    "                if urban:\n",
    "                    if r < 0.01:\n",
    "                        for Pr_L in loss_curves:\n",
    "                            yearly_loss_values.append(interpolate_damages(rps, loss_curves[Pr_L], r, protection_level))\n",
    "                    else:\n",
    "                        for Pr_L in urban_loss_curves:\n",
    "                            if Pr_L > r:\n",
    "                                yearly_loss_values.append(interpolate_damages(rps, urban_loss_curves[Pr_L], r, protection_level))\n",
    "                else:\n",
    "                    for Pr_L in loss_curves:\n",
    "                        if Pr_L > r:\n",
    "                            yearly_loss_values.append(interpolate_damages(rps, loss_curves[Pr_L], r, protection_level))        \n",
    "            national_losses_per_year[sim, year] = sum(yearly_loss_values)\n",
    "    return pd.DataFrame(national_losses_per_year, columns=[f'Year_{i+1}' for i in range(num_years)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca24cb47-4f76-4b05-b4cf-b512419fa114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 57/1000 [00:01<00:28, 32.80it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m baseline_losses \u001b[38;5;241m=\u001b[39m \u001b[43mmonte_carlo_dependence_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrisk_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHB_L6\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mToday\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBaseline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCombined\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 48\u001b[0m, in \u001b[0;36mmonte_carlo_dependence_simulation\u001b[0;34m(loss_df, rps, basin_col, epoch_val, scenario_val, urban_class, protection_level, num_years, urban, num_simulations)\u001b[0m\n\u001b[1;32m     46\u001b[0m urban_loss_curves \u001b[38;5;241m=\u001b[39m urban_basin_loss_curves[basin_id]\n\u001b[1;32m     47\u001b[0m loss_curves \u001b[38;5;241m=\u001b[39m basin_loss_curves[basin_id]\n\u001b[0;32m---> 48\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[43mrandom_ns\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbasin_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m urban:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.01\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Oxford/PRISK/.env/lib/python3.10/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/Documents/Oxford/PRISK/.env/lib/python3.10/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/Documents/Oxford/PRISK/.env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3777\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m other, result_name\n\u001b[1;32m   3774\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m   3775\u001b[0m \u001b[38;5;66;03m# Indexing Methods\u001b[39;00m\n\u001b[0;32m-> 3777\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_loc\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   3778\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3779\u001b[0m \u001b[38;5;124;03m    Get integer location, slice or boolean mask for requested label.\u001b[39;00m\n\u001b[1;32m   3780\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;124;03m    array([False,  True, False,  True])\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m     casted_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_indexer(key)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "baseline_losses = monte_carlo_dependence_simulation(risk_data, rps, 'HB_L6', 'Today', 'Baseline', 'Combined', 0.5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb41cea-b2cf-4614-9f3e-f123ffc81e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_emission_losses = monte_carlo_dependence_simulation(risk_data, rps, 'HB_L6', 'Future_High_Emission', 'Baseline', 'Combined', 0.5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c925e38-e165-4955-92de-ee4254d3c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_emission_losses = monte_carlo_dependence_simulation(risk_data, rps, 'HB_L6', 'Future_Low_Emission', 'Baseline', 'Combined', 0.5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd488313-d42f-4f64-a9c3-c8bc985becc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_protection_high_emission = monte_carlo_dependence_simulation(risk_data, rps, 'HB_L6', 'Future_High_Emission', 'Urban_Protection_RP100', 'Combined', 0.5, 100, urban=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f54de1-3fc0-4208-b517-14bf48020e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "dry_proofing_high_emission = monte_carlo_dependence_simulation(risk_data, rps, 'HB_L6', 'Future_High_Emission', 'Dry_Proofing', 'Combined', 0.5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f782e99-7024-4c7d-96cd-b63db5adeb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "relocation_high_emission = monte_carlo_dependence_simulation(risk_data, rps, 'HB_L6', 'Future_High_Emission', 'Relocation', 'Combined', 0.5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e0afee8-f099-4326-bb59-59cea0d4478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss-probability curve\n",
    "baseline_all_losses = baseline_losses.values.flatten()  # Flatten to get a single array of losses\n",
    "baseline_sorted_losses = np.sort(baseline_all_losses)[::-1]  # Sort losses from highest to lowest\n",
    "low_emission_all_losses = low_emission_losses.values.flatten()  # Flatten to get a single array of losses\n",
    "low_emission_sorted_losses = np.sort(low_emission_all_losses)[::-1]  # Sort losses from highest to lowest\n",
    "high_emission_all_losses = high_emission_losses.values.flatten()  # Flatten to get a single array of losses\n",
    "high_emission_sorted_losses = np.sort(high_emission_all_losses)[::-1]  # Sort losses from highest to lowest\n",
    "urban_protection_high_emission_all_losses = urban_protection_high_emission.values.flatten()  # Flatten to get a single array of losses\n",
    "urban_protection_high_emission_sorted_losses = np.sort(urban_protection_high_emission_all_losses)[::-1]  # Sort losses from highest to lowest\n",
    "dry_proofing_high_emission_all_losses = dry_proofing_high_emission.values.flatten()  # Flatten to get a single array of losses\n",
    "dry_proofing_high_emission_sorted_losses = np.sort(dry_proofing_high_emission_all_losses)[::-1]  # Sort losses from highest to lowest\n",
    "relocation_high_emission_all_losses = relocation_high_emission.values.flatten()  # Flatten to get a single array of losses\n",
    "relocation_high_emission_sorted_losses = np.sort(relocation_high_emission_all_losses)[::-1]  # Sort losses from highest to lowest\n",
    "aeps = np.arange(1, len(baseline_sorted_losses) + 1) / len(baseline_sorted_losses)  # Calculate AEPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138dbc4a-d74f-4cde-9efc-56fee4147c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(aeps, baseline_sorted_losses, marker='o', linestyle='-', markersize=2, label='Baseline', alpha=0.5, color='black')\n",
    "plt.semilogx(aeps, high_emission_sorted_losses, marker='o', linestyle='-', markersize=2, label='Future High Emission', alpha=0.5, color=\"red\")\n",
    "plt.semilogx(aeps, low_emission_sorted_losses, marker='o', linestyle='-', markersize=2, label='Future Low Emission', alpha=0.5, color=\"blue\")\n",
    "plt.semilogx(aeps, urban_protection_high_emission_sorted_losses, marker='o', linestyle='-', markersize=2, label='Urban Protection', alpha=0.5, color=\"green\")\n",
    "plt.semilogx(aeps, dry_proofing_high_emission_sorted_losses, marker='o', linestyle='-', markersize=2, label='Dry Proofing', alpha=0.5, color=\"purple\")\n",
    "plt.semilogx(aeps, relocation_high_emission_sorted_losses, marker='o', linestyle='-', markersize=2, label='Relocation', alpha=0.5, color=\"orange\")\n",
    "plt.xlabel('Annual Exceedance Probability (AEP)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Exceedance Probability Curve')\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('loss_exceedance_probability_curve_vine.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d3fd30-3d37-43c7-8b08-7b66795708c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_losses_b = baseline_sorted_losses/1000000000\n",
    "high_emissions_losses_b = high_emission_sorted_losses/1000000000\n",
    "low_emissions_losses_b = low_emission_sorted_losses/1000000000\n",
    "urban_protection_losses_b = urban_protection_high_emission_sorted_losses/1000000000\n",
    "dry_proofing_losses_b = dry_proofing_high_emission_sorted_losses/1000000000\n",
    "relocation_losses_b = relocation_high_emission_sorted_losses/1000000000\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(aeps, baseline_losses_b, marker='.', linestyle='-', label='Baseline', color='black')\n",
    "plt.plot(aeps, high_emissions_losses_b, marker='.', linestyle='-', label='High emissions', color='red')\n",
    "plt.plot(aeps, urban_protection_losses_b, marker='.', linestyle='-', label='Urban Protection', color='green')\n",
    "plt.plot(aeps, dry_proofing_losses_b, marker='.', linestyle='-', label='Dry Proofing', color='purple')\n",
    "plt.plot(aeps, relocation_losses_b, marker='.', linestyle='-', label='Relocation', color='orange')\n",
    "plt.plot(aeps, low_emissions_losses_b, marker='.', linestyle='-', label='Low Emissions', color='blue')\n",
    "# plt.xscale('log')\n",
    "plt.ylabel('Loss ($b)')\n",
    "plt.xlabel('Probability')\n",
    "plt.xlim(0, 0.5)\n",
    "#plt.ylim(0, 200)\n",
    "plt.legend()\n",
    "plt.title('Loss Exceedance Probability Curve')\n",
    "plt.grid(True, which='both', ls='--')\n",
    "plt.savefig('loss_exceedance_probability_curve_2_vine.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "353c8ce1-2c71-4b27-a3af-daf1ddf07fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_for_rp(aeps, losses, rp):\n",
    "    target_aep = 1 / rp\n",
    "    # Find the closest AEP in the array to the target AEP and get the corresponding loss\n",
    "    idx = (np.abs(aeps - target_aep)).argmin()\n",
    "    return losses[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b69865-db98-43e6-98d7-92bb559a5a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(baseline_losses_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
