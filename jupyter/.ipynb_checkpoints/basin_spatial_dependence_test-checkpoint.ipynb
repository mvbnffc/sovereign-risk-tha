{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "#### This Notebook attemps to implement the the structured coupling approach described in the following paper https://onlinelibrary.wiley.com/doi/epdf/10.1111/risa.12382?saml_referrer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "#### The notebook is structured as following\n",
    "1. Download river flow data for thailand\n",
    "2. Perform Data Checks\n",
    "3. Extract discharge timeseries at basin outlet points\n",
    "4. Fit Extreme Value Distribution (Gumbel)\n",
    "5. Transform data to Uniform Marginals\n",
    "6. Calculate pairwise dependence using the inverse Clayton copula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### 1. Download river flow data for Thailand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Functions\n",
    "import os\n",
    "import xarray as xr\n",
    "import netCDF4\n",
    "# CDS API\n",
    "import cdsapi\n",
    "# Disable warnings for data download via API\n",
    "import urllib3 \n",
    "urllib3.disable_warnings()\n",
    "# Disable xarray runtime warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up CDS API key\n",
    "\n",
    "if os.path.isfile(\"C:/Users/Mark.DESKTOP-UFHIN6T/.cdsapirc\"):\n",
    "    cdsapi_kwargs = {}\n",
    "else:\n",
    "    URL = 'https://cds.climate.copernicus.eu/api/v2'\n",
    "    KEY = '##################################'\n",
    "    cdsapi_kwargs = {\n",
    "        'url': URL,\n",
    "        'key': KEY,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data directory\n",
    "DATADIR = r\"D:\\projects\\sovereign-risk\\Thailand\\data\\flood\\dependence\\glofas\"\n",
    "os.makedirs(DATADIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data for Thaiand (the GIRI model uses the historical time period 1979-2016). We will pull data from 1979-2023\n",
    "start_year = 1979\n",
    "end_year = 2023\n",
    "# new_dir = os.path.join(DATADIR, f\"THA_{start_year}-{end_year}\")\n",
    "# os.makedirs(new_dir)\n",
    "c = cdsapi.Client(\n",
    "    **cdsapi_kwargs\n",
    "    )\n",
    "for year in range(start_year, end_year+1):\n",
    "    download_file = f\"{DATADIR}/glofas_THA_{year}.grib\"\n",
    "    if not os.path.isfile(download_file):\n",
    "        request_params = {\n",
    "            'system_version': 'version_4_0',\n",
    "            'hydrological_model': 'lisflood',\n",
    "            'product_type': 'consolidated',\n",
    "            'variable': 'river_discharge_in_the_last_24_hours',\n",
    "            'hyear': [f\"{year}\"],\n",
    "            'hmonth': ['january', 'february', 'march', 'april', 'may', 'june', \n",
    "                       'july', 'august', 'september', 'october', 'november', 'december'],\n",
    "            'hday': [f\"{day:02d}\" for day in range(1,31)],\n",
    "            'format': 'grib',\n",
    "            'area': [21, 97, 5, 106], # slightly larger bounding box than Thailand\n",
    "        }\n",
    "        c.retrieve('cems-glofas-historical', request_params).download(download_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the upstream area\n",
    "# NOTE: issue downloading a valid netcdf in current script. Workaround at the moment is using file I've previously downloaded\n",
    "upstream_area_fname = f\"uparea_glofas_v4_0.nc\"\n",
    "upstream_area_file = os.path.join(DATADIR, upstream_area_fname)\n",
    "# If we have not already downloaded the data, download it.\n",
    "if not os.path.isfile(upstream_area_file):\n",
    "    u_version=2 # file version\n",
    "    upstream_data_url = (\n",
    "        f\"https://confluence.ecmwf.int/download/attachments/242067380/{upstream_area_file}?\"\n",
    "        f\"version{u_version}&modificationDate=1668604690076&api=v2&download=true\"\n",
    "    )\n",
    "    import requests\n",
    "    result = requests.get(upstream_data_url)\n",
    "    with open(upstream_area_file, 'wb') as f:\n",
    "        f.write(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_datasets(start, end, dir):\n",
    "    '''\n",
    "    combine all datasets into one xarray for analysis\n",
    "    '''\n",
    "    all_files = [os.path.join(dir, f\"glofas_THA_{year}.grib\") for year in range(start, end+1)]\n",
    "    # Load all datasets into array\n",
    "    datasets = [xr.open_dataset(file, engine='cfgrib') for file in all_files]\n",
    "    # Concatenate all datasets along the time dimension\n",
    "    combined_dataset = xr.concat(datasets, dim='time')\n",
    "    # Make sure datasets are sorted by time\n",
    "    combined_dataset = combined_dataset.sortby('time')\n",
    "    \n",
    "    return combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load glofas data and combine\n",
    "glofas_data = combine_datasets(start_year, end_year, DATADIR)\n",
    "# glofas_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the Upstream area data to the domain of the river discharge\n",
    "\n",
    "# Filter glofas timeseries based on upstream accumulating area\n",
    "area_filter = 500\n",
    "\n",
    "# Open the file and print the contents\n",
    "upstream_area = xr.open_dataset(upstream_area_file, engine='netcdf4')\n",
    "\n",
    "# Get the latitude and longitude limits of the data\n",
    "lat_limits = [glofas_data.latitude.values[i] for i in [0, -1]]\n",
    "lon_limits = [glofas_data.longitude.values[i] for i in [0, -1]]\n",
    "up_lats = upstream_area.latitude.values.tolist()\n",
    "up_lons = upstream_area.longitude.values.tolist()\n",
    "\n",
    "lat_slice_index = [\n",
    "    round((i-up_lats[0])/(up_lats[1]-up_lats[0]))\n",
    "    for i in lat_limits\n",
    "]\n",
    "lon_slice_index = [\n",
    "    round((i-up_lons[0])/(up_lons[1]-up_lons[0]))\n",
    "    for i in lon_limits\n",
    "]\n",
    "\n",
    "# Slice upstream area to Thailand region:\n",
    "red_upstream_area = upstream_area.isel(\n",
    "    latitude=slice(lat_slice_index[0], lat_slice_index[1]+1),\n",
    "    longitude=slice(lon_slice_index[0], lon_slice_index[1]+1),\n",
    ")\n",
    "\n",
    "# There are very minor rounding differences, so we update with the lat/lons from the glofas data\n",
    "red_upstream_area = red_upstream_area.assign_coords({\n",
    "    'latitude': glofas_data.latitude,\n",
    "    'longitude': glofas_data.longitude,\n",
    "})\n",
    "\n",
    "# Add the upstream area to the main data object and print the updated glofas data object:\n",
    "glofas_data['uparea'] = red_upstream_area['uparea']\n",
    "glofas_data\n",
    "\n",
    "# Mask the river discharge data\n",
    "glofas_data_masked = glofas_data.where(glofas_data.uparea>=area_filter*1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glofas_data_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### 2. Perform data checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the basin outlet data\n",
    "basin_outlet_file = r\"D:\\projects\\sovereign-risk\\Thailand\\data\\flood\\dependence\\thailand-basins\\lev06_outlets_final_clipped_Thailand.csv\"\n",
    "basin_outlet_df = pd.read_csv(basin_outlet_file)\n",
    "# Note to align the two datasets we need to make the following adjustment to lat lons (based on previous trial and error)\n",
    "basin_outlet_df['Latitude'] = basin_outlet_df['Latitude'] + 0.05/2\n",
    "basin_outlet_df['Longitude'] = basin_outlet_df['Longitude'] - 0.05/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for checking timeseries\n",
    "def check_timeseries(array, latitude, longitude):\n",
    "    test_point = array.sel(latitude=latitude, longitude=longitude, method='nearest')\n",
    "    test_timeseries = test_point['dis24']\n",
    "    test_acc = float(test_point['uparea'])\n",
    "    # check for NaN values\n",
    "    non_nan_count = test_timeseries.count().item()\n",
    "    total_count = test_timeseries.size\n",
    "    nan_ratio = non_nan_count/total_count\n",
    "\n",
    "    # Does the timeseries pass the NaN threshold\n",
    "    if nan_ratio < 1:\n",
    "        return False, test_acc, \"NaN values found\"\n",
    "\n",
    "    # Check for constant values\n",
    "    if test_timeseries.min() == test_timeseries.max():\n",
    "        return False, test_acc, \"Constant timeseries values\"\n",
    "\n",
    "    # If all checks pass\n",
    "    return True, test_acc, \"Valid timeseries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through basins and check whether timeseries is valid\n",
    "results = []\n",
    "for index, row in basin_outlet_df.iterrows():\n",
    "    latitude = row['Latitude']\n",
    "    longitude = row['Longitude']\n",
    "\n",
    "    valid, acc, message = check_timeseries(glofas_data_masked, latitude, longitude)\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'HYBAS_ID': row['HYBAS_ID'],\n",
    "        'Latitude': latitude,\n",
    "        'Longitude': longitude,\n",
    "        'Acc': acc,\n",
    "        'Valid': valid,\n",
    "        'Message': message\n",
    "    })\n",
    "    if not valid:\n",
    "        print(f\"ID: {row['HYBAS_ID']}, Lat: {latitude}, Lon: {longitude}, Acc: {acc}, Valid: {valid}, Message: {message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### 3. Extract discharge timeseries at basin outlet points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# over what years do we want to extract the data?\n",
    "start_year = 1979\n",
    "end_year = 2016\n",
    "sliced_data = glofas_data_masked.sel(time=slice(str(start_year), str(end_year)))\n",
    "# Dictionary to store timeseries data for each basin\n",
    "basin_timeseries = {}\n",
    "\n",
    "# Loop through basin outlets, storing each in turn\n",
    "for index, row in basin_outlet_df.iterrows():\n",
    "    basin_id = row['HYBAS_ID']\n",
    "    lat = row['Latitude']\n",
    "    lon = row['Longitude']\n",
    "    point_data = sliced_data.sel(latitude=lat, longitude=lon, method='nearest')\n",
    "    timeseries = point_data['dis24'].to_series()\n",
    "    # store in dictionary\n",
    "    basin_timeseries[basin_id] = timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Step 4: Fit Extreme Value Distribution (Gumbel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from lmoments3 import distr\n",
    "from scipy.stats import gumbel_r, kstest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store fitted parameters for each basin\n",
    "gumbel_params = {}\n",
    "fit_quality = {}\n",
    "\n",
    "# Loop through basins, calculating annual maxima and fitting Gumbel distribution using L-moments\n",
    "for basin_id, timeseries in basin_timeseries.items():\n",
    "    annual_maxima = timeseries.groupby(timeseries.index.year).max()\n",
    "\n",
    "    # Fit Gumbel distribution using L-moments\n",
    "    params = distr.gum.lmom_fit(annual_maxima)\n",
    "\n",
    "    # Perform the Kolmogorov-Smirnov test (checking quality of fit)\n",
    "    D, p_value = kstest(annual_maxima, 'gumbel_r', args=(params['loc'], params['scale']))\n",
    "\n",
    "    gumbel_params[basin_id] = params\n",
    "    fit_quality[basin_id] = (D, p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check fit visually\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from scipy.stats import gumbel_r\n",
    "\n",
    "# for basin_id, timeseries in basin_timeseries.items():\n",
    "#     annual_maxima = timeseries.groupby(timeseries.index.year).max()\n",
    "#     params = gumbel_params[basin_id]\n",
    "#     loc, scale = params['loc'], params['scale']\n",
    "#     # # Debug plot\n",
    "    \n",
    "#     plt.hist(annual_maxima, bins=30, density=True, alpha=0.5, color='g')\n",
    "#     xmin, xmax = plt.xlim()\n",
    "#     x = np.linspace(xmin, xmax, 100)\n",
    "#     p = gumbel_r.pdf(x, loc, scale)\n",
    "#     plt.plot(x, p, 'k', linewidth=2)\n",
    "#     plt.title(\"Histogram with Fitted Gumbel PDF %s\" % basin_id)\n",
    "#     plt.show()\n",
    "\n",
    "#     plt.figure(figsize=(10, 6))  # Adjust the size of your plot\n",
    "#     plt.plot(time_series.index, time_series.values)\n",
    "#     plt.title('Time Series Plot')\n",
    "#     plt.xlabel('Time')\n",
    "#     plt.ylabel('dis24 Values')\n",
    "#     plt.xticks(rotation=45)  # Rotate the x-axis labels for better readability\n",
    "#     plt.tight_layout()  # Adjust layout to make room for the rotated x-axis labels\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Step 5: Transform data to Uniform Marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will do this using the CDF of the fitted Gumbel distribution \n",
    "\n",
    "# Dictionary to story uniform marginals for each basin\n",
    "uniform_marginals = {}\n",
    "\n",
    "for basin_id, timeseries in basin_timeseries.items():\n",
    "    annual_maxima = timeseries.groupby(timeseries.index.year).max()\n",
    "    params = gumbel_params[basin_id]\n",
    "    uniform_marginals[basin_id] = gumbel_r.cdf(annual_maxima, loc=params['loc'], scale=params['scale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check fit visually\n",
    "\n",
    "# for basin_id, timeseries in basin_timeseries.items():\n",
    "#     annual_maxima = timeseries.groupby(timeseries.index.year).max()\n",
    "#     params = gumbel_params[basin_id]\n",
    "#     # Sort data for empirical CDF plot\n",
    "#     sorted_maxima = np.sort(annual_maxima)\n",
    "#     empirical_cdf = np.arange(1, len(sorted_maxima)+1) / len(sorted_maxima)\n",
    "    \n",
    "#     # Theoretical CDF\n",
    "#     theoretical_cdf = gumbel_r.cdf(sorted_maxima, loc=params['loc'], scale=params['scale'])\n",
    "\n",
    "#     # Plotting\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(sorted_maxima, empirical_cdf, label='Empirical CDF')\n",
    "#     plt.plot(sorted_maxima, theoretical_cdf, label='Theoretical CDF', linestyle='--')\n",
    "#     plt.title(f'ECDF vs Theoretical CDF for Basin {basin_id}')\n",
    "#     plt.xlabel('Annual Maxima Discharge')\n",
    "#     plt.ylabel('CDF')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Step 6: Calculate pairwise dependence using the inverse Clayton copula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# NOTE: need to change virual environment to copulas here (issue with dependencies for sovereign-risk env)\n",
    "from copulas.bivariate import Clayton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store copula models for each pair of basins\n",
    "clayton_copula_models = {}\n",
    "clayton_error_basins = [] # list to store basins that cause an error\n",
    "\n",
    "for id1, margins1 in uniform_marginals.items():\n",
    "    for id2, margins2 in uniform_marginals.items():\n",
    "        if id1 < id2: # to avoid duplicate pairs\n",
    "            try:\n",
    "                # Prepare the data for copula\n",
    "                data = np.column_stack((1-margins1, 1-margins2)) # interested in upper tail dependence so take inverse of CDF\n",
    "                \n",
    "                # Fit the Clayton copula\n",
    "                flipped_clayton = Clayton()\n",
    "                flipped_clayton.fit(data)\n",
    "    \n",
    "                # Store the copula model\n",
    "                clayton_copula_models[(id1, id2)] = flipped_clayton\n",
    "            except ValueError as e:\n",
    "                # print(f\"Error fitting Clayton copula for basins {id1} and {id2}: {e}\")\n",
    "                clayton_error_basins.append((id1, id2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store these copula pairs in a matrix\n",
    "\n",
    "basin_ids = list(uniform_marginals.keys()) # take the basin IDs from the uniform marginals dictionary\n",
    "N = len(basin_ids)\n",
    "\n",
    "\n",
    "# Initialize the matrix with NaNs\n",
    "dependence_matrix = np.full((N, N), np.nan)\n",
    "\n",
    "# Map from basin ID to matrix index\n",
    "id_to_index = {basin_id: index for index, basin_id in enumerate(basin_ids)}\n",
    "\n",
    "for (id1, id2), copula_model in clayton_copula_models.items():\n",
    "    index1, index2 = id_to_index[id1], id_to_index[id2]\n",
    "    dependence_matrix[index1, index2] = copula_model.theta\n",
    "    dependence_matrix[index2, index1] = copula_model.theta\n",
    "\n",
    "# For error basins do the same but set theta to -1\n",
    "for (id1, id2) in clayton_error_basins:\n",
    "    index1, index2 = id_to_index[id1], id_to_index[id2]\n",
    "    dependence_matrix[index1, index2] = -1\n",
    "    dependence_matrix[index2, index1] = -1\n",
    "\n",
    "# Debug (for infinity values) - not sure if needed but there are a few where I had to reassign basin outlets.\n",
    "dependence_matrix[np.isinf(dependence_matrix)] = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(dependence_matrix, cmap='viridis', interpolation='none', vmin=0, vmax=2)\n",
    "plt.colorbar()\n",
    "plt.title('Basin Dependence Structure')\n",
    "plt.xlabel('Basin Index')\n",
    "plt.ylabel('Basin Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Step 7: Ordered Coupling for Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "##### Following the minimax structuring approach described in the Timonina et al (2015) paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Find the most dependent pair\n",
    "# Initialize a set to keep track of selected basin indices\n",
    "selected_indices = set()\n",
    "# convert dependence_matrix to a masked array, so that NaN values and -1 are not considered in the operation\n",
    "masked_dependence_matrix = np.ma.masked_less(dependence_matrix, 0) # masking out values < 0\n",
    "np.fill_diagonal(masked_dependence_matrix, np.ma.masked) # we want to ignore diagonal (NaN values)\n",
    "max_theta_index = np.unravel_index(np.argmax(masked_dependence_matrix, axis=None), masked_dependence_matrix.shape)\n",
    "ordered_basins = [basin_ids[max_theta_index[0]], basin_ids[max_theta_index[1]]]\n",
    "# Add indices to the set of selected indices\n",
    "selected_indices.update([max_theta_index[0], max_theta_index[1]])\n",
    "\n",
    "# Step 2-4: Loop until all basins are ordered\n",
    "while len(ordered_basins) < len(basin_ids):\n",
    "    # Step 2: Choose basin k that is dependent on both basin i, j (last two basins in ordered_basins). Minimax approach\n",
    "    # Exclude already selected basins from the selection process\n",
    "    potential_next_indices = [i for i in range(len(basin_ids)) if i not in selected_indices]\n",
    "    # Find the indices of the last two basins in ordered_basins\n",
    "    last_two_indices = [id_to_index[basin] for basin in ordered_basins[-2:]]\n",
    "    # Find dependency vectors for the last two basins\n",
    "    dependency_vectors = masked_dependence_matrix[last_two_indices, :]\n",
    "    # Calculate the minimum dependency for each row of the vector\n",
    "    min_deps = np.ma.min(dependency_vectors, axis=0)\n",
    "    # Mask already selected indices\n",
    "    min_deps_masked = np.ma.copy(min_deps)\n",
    "    for idx in selected_indices:\n",
    "        min_deps_masked.mask[idx] = True # mask the index if it's already in selected indices\n",
    "    # Step 3: Find the maximum dependency value over the minimized vector - which will be the next basin\n",
    "    next_basin_index = np.ma.argmax(min_deps_masked, fill_value=-np.inf)\n",
    "    # Step 4: Continue iterations until there are no more basins left to process\n",
    "    # Check if all options are effectively masked\n",
    "    if min_deps_masked.mask.all():\n",
    "        print(\"No suitable next basin found. Ending process.\")\n",
    "        break\n",
    "    next_basin = basin_ids[next_basin_index]\n",
    "    ordered_basins.append(next_basin)\n",
    "    selected_indices.add(next_basin_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Step 8: Aggregate through Hierarchical Monte-Carlo Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_conditional_sample(v, theta, r):\n",
    "    '''\n",
    "    Generate a conditional sample using the Flipped Calyton copula.\n",
    "    Equation 12 from the Timonina et al (2015) paper\n",
    "\n",
    "    :param v: Known loss in basin i\n",
    "    :param theta: Copula parameter for dependency between basins i and j.\n",
    "    :param r: Random value from uniform distribution for sampling.\n",
    "    :retrun: Generated conditional loss in basin j.\n",
    "    '''\n",
    "    u = 1-(1+((1-v)**(-theta))*(((r**(-((theta)/(1+theta))))-1)))**(-(1/theta))\n",
    "    return u "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting loss-probability curve for basin\n",
    "def basin_loss_curve(loss_df, basin_id, basin_col, epoch_val, scenario_val, rps):\n",
    "    losses = [] # initialize empty list to store losses\n",
    "    basin_df = loss_df[(loss_df[basin_col]==basin_id) & (loss_df['epoch']==epoch_val) & (loss_df['adaptation_scenario']==scenario_val)]\n",
    "    grouped_basin_df = basin_df.groupby([basin_col, 'RP']).agg({'damages':'sum'}).reset_index()\n",
    "    for i in rps:\n",
    "        losses.append(grouped_basin_df.loc[grouped_basin_df['RP'] == i, 'damages'].sum())\n",
    "    # Return losses (index indicates what the RP is)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_damages(RPs, losses, sim_aep):\n",
    "    aeps = [1/i for i in RPs]\n",
    "    # Ensure AEPs are in ascending order for np.interp\n",
    "    aeps.sort() \n",
    "    losses = losses[::-1]\n",
    "\n",
    "    # Interpolate based off simulated AEP\n",
    "    if sim_aep >= 0.5: \n",
    "        return 0 \n",
    "    else:\n",
    "        interpolated_value = np.interp(sim_aep, aeps, losses)\n",
    "        return interpolated_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_copula_model(copula_models, basin1, basin2):\n",
    "    \"\"\"\n",
    "    Attempt to retrieve a copula model for a given pair of basins.\n",
    "    Tries both possible orders of the basin IDs.\n",
    "\n",
    "    :param copula_models: Dictionary of copula models.\n",
    "    :param basin1: ID of the first basin.\n",
    "    :param basin2: ID of the second basin.\n",
    "    :return: The copula model if found, else None.\n",
    "    \"\"\"\n",
    "    return copula_models.get((basin1, basin2)) or copula_models.get((basin2, basin1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Monte Carlo simulating incorporating basin dependencies\n",
    "def monte_carlo_dependence_simulation(loss_df, rps, basin_col, epoch_val, scenario_val, num_years, ordered_basins, copula_models, gumbel_params, num_simulations=10000):\n",
    "    '''\n",
    "    Perform Monte Carlo simulations of yearly losses incorporating basin dependencies.\n",
    "\n",
    "    :param loss_df: dataframe with losses from risk analysis\n",
    "    :param rps: list of return periods to consider. \n",
    "    :param basin_col: name of column for basins (e.g. 'HB_L6')\n",
    "    :param epoch_val: name of epoch value (e.g. 'Today')\n",
    "    :param scenario_val: name of scenario (e.g. 'Baseline')\n",
    "    :param num_years: Number of years to simulate\n",
    "    :param ordered_basins: List of basin IDs ordered by dependency\n",
    "    :param copula_models: Dictionary holding copula model for each basin pair\n",
    "    :param gumbel_params: Gumbel distribution parameters for each basin.\n",
    "    :param num__simulations: Number of simulations (default is 10,000).\n",
    "    :return: Dataframe of simulated national losses for each year.\n",
    "    '''\n",
    "\n",
    "    # To speed up the Monte-Carlo simulation we are going to pre-compute some variables\n",
    "    # precompute loss-probability curves for each basin\n",
    "    basin_loss_curves = {basin_id: basin_loss_curve(loss_df, basin_id, basin_col, epoch_val, scenario_val, rps) for basin_id in ordered_basins}\n",
    "    # Initialize arrat for national losses\n",
    "    national_losses_per_year = np.zeros((num_simulations, num_years))\n",
    "    # Generate all random numbers in advance\n",
    "    random_numbers = np.random.uniform(0, 1, (num_simulations, num_years, len(ordered_basins))).astype(np.float32)\n",
    "\n",
    "    for simulation in range(num_simulations):\n",
    "        # print progress\n",
    "        if simulation % 50 == 0:\n",
    "            print('Simulation progress: %s out of %s' % (simulation, num_simulations))\n",
    "        for year in range(num_years):\n",
    "            # Initialize a list to store losses for each basin for the current year\n",
    "            yearly_losses = []\n",
    "            yearly_loss_values = []\n",
    "            for i, basin_id in enumerate(ordered_basins):\n",
    "                if i == 0:\n",
    "                    # Handle first basin\n",
    "                    r = random_numbers[simulation, year, i]\n",
    "                    loss_curve = basin_loss_curves[basin_id]\n",
    "                    yearly_losses.append(r) # will store losses as AEP\n",
    "                    yearly_loss_values.append(interpolate_damages(rps, loss_curve, r)) # store losses as interpolated values\n",
    "                else:\n",
    "                    loss_curve = basin_loss_curves[basin_id]\n",
    "                    # Handle subsequent basins with dependencies\n",
    "                    copula = get_copula_model(copula_models, ordered_basins[i-1], basin_id)\n",
    "                    if copula is not None:\n",
    "                        # Apply dependency model if theta exists\n",
    "                        r = random_numbers[simulation, year, i]\n",
    "                        previous_loss = yearly_losses[i-1]\n",
    "                        current_loss = generate_conditional_sample(previous_loss, copula.theta, r)\n",
    "                        yearly_losses.append(current_loss)\n",
    "                        yearly_loss_values.append(interpolate_damages(rps, loss_curve, r))\n",
    "                    else:\n",
    "                        # Independent simulation for this basin\n",
    "                        r = random_numbers[simulation, year, i]\n",
    "                        yearly_losses.append(r)\n",
    "                        yearly_loss_values.append(interpolate_damages(rps, loss_curve, r))\n",
    "\n",
    "            # Aggregate losses for the current year\n",
    "            national_losses_per_year[simulation, year] = sum(yearly_loss_values)\n",
    "\n",
    "    # Convert the results into a DataFrame\n",
    "    return pd.DataFrame(national_losses_per_year, columns=[f'Year_{i+1}' for i in range(num_years)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data = pd.read_csv('risk_basin_simplified.csv')\n",
    "rps = [2, 5, 10, 25, 50, 100, 200, 500, 1000]\n",
    "simulated_losses = monte_carlo_dependence_simulation(loss_data, rps, 'HB_L6', 'Today', 'Baseline', 1000, ordered_basins, clayton_copula_models, gumbel_params, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = monte_carlo_dependence_simulation(loss_data, rps, 'HB_L6', 'Today', 'Baseline', 1, ordered_basins, clayton_copula_models, gumbel_params, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent and dependent loss functions\n",
    "def simulate_independent_losses(loss_df, rps, num_years, ordered_basins, num_simulations=10000):\n",
    "    basin_loss_curves = {basin_id: basin_loss_curve(loss_df, basin_id, 'HB_L6', 'Today', 'Baseline', rps) for basin_id in ordered_basins}\n",
    "    national_losses_per_year = np.zeros((num_simulations, num_years))\n",
    "    \n",
    "    for simulation in range(num_simulations):\n",
    "        for year in range(num_years):\n",
    "            yearly_loss_values = [interpolate_damages(rps, basin_loss_curves[basin_id], np.random.uniform(0, 1)) for basin_id in ordered_basins]\n",
    "            national_losses_per_year[simulation, year] = sum(yearly_loss_values)\n",
    "    \n",
    "    return national_losses_per_year.flatten()  # Flatten to create a single array of all losses\n",
    "def simulate_completely_dependent_losses(loss_df, rps, num_years, ordered_basins, num_simulations=10000):\n",
    "    # Arbitrarily use the first basin's loss curve as the basis for dependence\n",
    "    first_basin_loss_curve = basin_loss_curve(loss_df, ordered_basins[0], 'HB_L6', 'Today', 'Baseline', rps)\n",
    "    national_losses_per_year = np.zeros((num_simulations, num_years))\n",
    "    \n",
    "    for simulation in range(num_simulations):\n",
    "        for year in range(num_years):\n",
    "            # Simulate one loss and apply it across all basins\n",
    "            r = np.random.uniform(0, 1)\n",
    "            common_loss_value = interpolate_damages(rps, first_basin_loss_curve, r)\n",
    "            national_losses_per_year[simulation, year] = common_loss_value * len(ordered_basins)  # Multiply by the number of basins for total loss\n",
    "    \n",
    "    return national_losses_per_year.flatten()  # Flatten to create a single array of all losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_losses = simulate_independent_losses(loss_data, rps, 1000, ordered_basins, num_simulations=500)\n",
    "dependent_losses = simulate_completely_dependent_losses(loss_data, rps, 1000, ordered_basins, num_simulations=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss-probability curve\n",
    "all_losses =simulated_losses.values.flatten()  # Flatten to get a single array of losses\n",
    "sorted_losses = np.sort(all_losses)[::-1]  # Sort losses from highest to lowest\n",
    "aeps = np.arange(1, len(sorted_losses) + 1) / len(sorted_losses)  # Calculate AEPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(aeps, sorted_losses, marker='o', linestyle='-', markersize=2)\n",
    "plt.semilogx(aeps, np.sort(independent_losses)[::-1], marker='o', linestyle='-', markersize=2)\n",
    "plt.xlabel('Annual Exceedance Probability (AEP)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Exceedance Probability Curve')\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other approach for dependent losses\n",
    "def get_losses_for_rps(loss_df, basin_col, epoch_val, scenario_val, rps, ordered_basins):\n",
    "    \"\"\"\n",
    "    Calculate the losses for each basin for specific return periods.\n",
    "    \"\"\"\n",
    "    losses_for_rps = {}\n",
    "    for basin_id in ordered_basins:\n",
    "        loss_curve = basin_loss_curve(loss_df, basin_id, basin_col, epoch_val, scenario_val, rps)\n",
    "        losses_for_rps[basin_id] = loss_curve\n",
    "    return losses_for_rps\n",
    "def aggregate_losses_nationally(losses_for_rps, rps, ordered_basins):\n",
    "    \"\"\"\n",
    "    Aggregate the losses for each return period across all basins.\n",
    "    \"\"\"\n",
    "    national_losses = {}\n",
    "    for rp in rps:\n",
    "        total_loss = sum(losses_for_rps[basin_id][rp] for basin_id in ordered_basins)\n",
    "        national_losses[rp] = total_loss\n",
    "    return national_losses\n",
    "def prepare_data_for_plotting(national_losses):\n",
    "    \"\"\"\n",
    "    Prepare the national loss data for plotting.\n",
    "    \"\"\"\n",
    "    aeps = [1/rp for rp in national_losses.keys()]\n",
    "    losses = list(national_losses.values())\n",
    "    return aeps, losses\n",
    "losses_for_rps = get_losses_for_rps(loss_df, 'HB_L6', 'Today', 'Baseline', rps, ordered_basins)\n",
    "national_losses = aggregate_losses_nationally(losses_for_rps, rps, ordered_basins)\n",
    "dep_aeps, dep_losses = prepare_data_for_plotting(national_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_losses_b = sorted_losses/1000000000\n",
    "independent_losses_b = np.sort(independent_losses/1000000000)[::-1]\n",
    "dependent_losses_b = np.sort(dependent_losses/1000000000)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(aeps, theta_losses_b, marker='.', linestyle='-', label='dependent losses')\n",
    "plt.plot(aeps, independent_losses_b, marker='.', linestyle='-', label='indepentent losses')\n",
    "# plt.plot(aeps, dependent_losses_b, marker='.', linestyle='-', label='completely dependent losses')\n",
    "# plt.xscale('log')\n",
    "plt.ylabel('Loss ($b)')\n",
    "plt.xlabel('Probability')\n",
    "plt.xlim(0, 0.5)\n",
    "plt.suptitle('Loss Exceedance Probability Curve')\n",
    "plt.grid(True, which='both', ls='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_theta_values = {}\n",
    "for (basin1, basin2), model in clayton_copula_models.items():\n",
    "    extracted_theta_values[(basin1, basin2)] = model.theta  # Assuming the copula model has a .theta attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_for_rp(aeps, losses, rp):\n",
    "    target_aep = 1 / rp\n",
    "    # Find the closest AEP in the array to the target AEP and get the corresponding loss\n",
    "    idx = (np.abs(aeps - target_aep)).argmin()\n",
    "    return losses[idx]\n",
    "\n",
    "# Example: Get the 100-year RP loss\n",
    "loss_100yr = get_loss_for_rp(aeps, sorted_losses, 100)\n",
    "loss_10yr = get_loss_for_rp(aeps, sorted_losses, 10)\n",
    "loss_2yr = get_loss_for_rp(aeps, sorted_losses, 2)\n",
    "print(f\"The 100-year RP loss is: {loss_100yr}\")\n",
    "print(f\"The 10-year RP loss is: {loss_10yr}\")\n",
    "print(f\"The 2-year RP loss is: {loss_2yr}\")\n",
    "loss_100yr = get_loss_for_rp(aeps, np.sort(independent_losses)[::-1], 100)\n",
    "loss_10yr = get_loss_for_rp(aeps, np.sort(independent_losses)[::-1], 10)\n",
    "loss_2yr = get_loss_for_rp(aeps, np.sort(independent_losses)[::-1], 2)\n",
    "print(f\"The 100-year RP loss is: {loss_100yr}\")\n",
    "print(f\"The 10-year RP loss is: {loss_10yr}\")\n",
    "print(f\"The 2-year RP loss is: {loss_2yr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### AREA FOR DEBUGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 0.5  # Fixed known loss in basin i\n",
    "thetas = [0.00001, 0.5, 1.0, 2.0, 10]  # Different values of theta to visualize\n",
    "r_samples = np.random.uniform(0, 1, 1000)  # Generate 1000 random samples\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "for theta in thetas:\n",
    "    u_samples = [generate_conditional_sample(v, theta, r) for r in r_samples]\n",
    "    plt.hist(u_samples, bins=50, alpha=0.5, label=f'Theta={theta}')\n",
    "\n",
    "plt.title('Effect of Varying Theta on Conditional Loss Distribution')\n",
    "plt.xlabel('Conditional Loss u')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_matrix = dependence_matrix[0:5, 0:5]\n",
    "\n",
    "plt.imshow(subset_matrix, cmap='viridis', interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.title('Subset of Basin Dependence Structure')\n",
    "plt.xlabel('Subset Basin Index')\n",
    "plt.ylabel('Subset Basin Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert matrix to dataframe\n",
    "dependence_matrix_df = pd.DataFrame(dependence_matrix, index=basin_ids, columns=basin_ids)\n",
    "# Export to cSV to inspect\n",
    "dependence_matrix_df.to_csv('dependence_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "copulas",
   "language": "python",
   "name": "copulas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
